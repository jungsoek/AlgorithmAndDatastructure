# 2. 알고리즘(Algorithms)

# 2.1 알고리즘 기본 개념

## 알고리즘의 정의 및 성능 기준

### 1. 알고리즘이란?

**알고리즘(Algorithm)**이란, **특정 문제를 해결하기 위한 일련의 명확하고 유한한 단계(step)**를 말한다.
 일상 언어나 컴퓨터 언어로 표현할 수 있으며, 실행 결과는 **명확하게 정의된 출력(output)**을 생성해야 한다.

> 📌 대표적 정의 (Donald Knuth):
>  “알고리즘은 입력을 받아 문제를 해결하고 출력을 생성하는 유한한 명령의 집합이다.”

### 2. 알고리즘의 필수 조건

| 조건             | 설명                                                        |
| ---------------- | ----------------------------------------------------------- |
| **입력(Input)**  | 0개 이상의 입력이 존재해야 함                               |
| **출력(Output)** | 최소 1개의 출력이 있어야 함                                 |
| **명확성**       | 각 단계는 모호하지 않고 명확해야 함 (Deterministic)         |
| **유한성**       | 알고리즘은 유한한 횟수의 단계 내에 반드시 종료되어야 함     |
| **효율성**       | 시간/공간 낭비 없이 목적을 달성해야 함 (최적화 가능성 존재) |

### 3. 알고리즘 성능 기준

알고리즘을 평가할 때는 주로 **시간 복잡도**와 **공간 복잡도** 두 가지 성능 지표를 사용한다.

#### 3.1 시간 복잡도 (Time Complexity)

> 알고리즘이 실행되는 데 소요되는 **기본 연산의 총 횟수**

- 입력 크기 $n$에 따라 증가하는 계산량
- 대표 표기: $O(1), O(\log n), O(n), O(n \log n), O(n^2)$ 등

예시:

```
for (int i = 0; i < n; i++) {
    printf("*");
}
```

→ 시간 복잡도: $O(n)$

#### 3.2 공간 복잡도 (Space Complexity)

> 알고리즘이 문제를 해결하는 데 사용되는 **메모리의 양**

- 입력 크기에 따라 사용하는 추가 메모리량을 평가
- 고정 배열, 재귀 깊이, 동적 자료구조 등이 영향을 줌

예시:

```
int arr[1000];  // O(n) 공간 사용
```

### 4. 점근적 표기법 (Asymptotic Notation)

입력이 커질수록 알고리즘이 얼마나 빠르게 실행 시간이 증가하는지를 수학적으로 표현한다.

| 표기법         | 의미               | 설명                             |
| -------------- | ------------------ | -------------------------------- |
| $O(f(n))$      | 상한 (최악의 경우) | 성능 보증용                      |
| $\Omega(f(n))$ | 하한 (최선의 경우) | 이보다 빨라지지 않음             |
| $\Theta(f(n))$ | 정확한 경계        | 평균적 또는 일반적인 경우 성장률 |

### 5. 알고리즘 성능 비교 예시

| 알고리즘            | 시간 복잡도 (최악)                              |
| ------------------- | ----------------------------------------------- |
| 선형 탐색           | $O(n)$                                          |
| 이진 탐색           | $O(\log n)$                                     |
| 버블 정렬           | $O(n^2)$                                        |
| 퀵 정렬             | $O(n^2)$                                        |
| 병합 정렬           | $O(n \log n)$                                   |
| 다익스트라 알고리즘 | $O(V^2)$ or $O(E \log V)$ (우선순위 큐 사용 시) |

### 6. 알고리즘의 올바름 (Correctness)

알고리즘은 반드시 **모든 입력에 대해 정해진 출력**을 내야 하며, 다음 두 가지를 만족해야 한다.

| 항목       | 설명                                               |
| ---------- | -------------------------------------------------- |
| **정확성** | 올바른 입력에 대해 항상 올바른 결과 출력           |
| **완전성** | 모든 유효 입력에 대해 예외 없이 종료되고 결과 생성 |

### 7. 알고리즘 분석의 목적

- 성능 예측 (실행 시간과 자원 사용량 측정)
- 최적의 알고리즘 선택
- 대규모 입력에 대한 확장성 확보
- 시간과 공간 간의 트레이드오프 평가

### 8. 알고리즘 설계 패러다임 (기초 개념)

| 패러다임                          | 설명                                     |
| --------------------------------- | ---------------------------------------- |
| **탐욕법(Greedy)**                | 현재 단계에서 최선 선택 → 전체 최적화    |
| **분할 정복(Divide and Conquer)** | 문제를 나눠 재귀적으로 해결 → 병합       |
| **동적 계획법(DP)**               | 부분 문제를 저장하며 해결 (메모이제이션) |
| **백트래킹**                      | 후보군 탐색 중 조건 미충족시 되돌아가기  |
| **분지한정(Branch & Bound)**      | 백트래킹에 최적화 가지치기 포함          |

### 9. 결론

- 알고리즘은 **정의, 절차, 유한성, 명확성**을 갖춘 문제 해결 도구다.
- 성능 분석을 통해 실행 시간과 자원 사용을 미리 예측할 수 있다.
- **좋은 알고리즘**은 단순히 실행 가능한 것이 아니라, **정확하고 효율적이며 최적화 가능**한 구조여야 한다.

## 시간복잡도 및 공간복잡도 계산

### 1. 시간복잡도(Time Complexity)란?

**시간복잡도**는 알고리즘이 실행되는 동안 **실행되는 연산의 수**를 **입력 크기 $n$**에 따라 추상화한 것이다.
 기계의 실행 시간이 아닌, **"연산 횟수의 증가율"**에 초점을 둔다.

#### 📌 기본 원칙

- 연산의 **가장 많은 반복이 일어나는 부분**에 주목한다.
- 상수는 무시하고 **성장률이 가장 큰 항**만 취급한다.
- 다중 반복문의 경우, **중첩 루프**는 곱셈, **연속 루프**는 덧셈으로 처리한다.

### 2. 시간복잡도 계산 예시 (C 코드 기반)

#### 2.1 단일 반복문

```
for (int i = 0; i < n; i++) {
    // O(1)
}
```

- 반복 횟수: $n$
- 시간복잡도: **O(n)**

#### 2.2 중첩 반복문

```
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        // O(1)
    }
}
```

- 총 반복 횟수: $n \times n = n^2$
- 시간복잡도: **O(n²)**

#### 2.3 삼중 루프

```
for (int i = 0; i < n; i++)
  for (int j = 0; j < n; j++)
    for (int k = 0; k < n; k++)
      doSomething();
```

- 반복 횟수: $n^3$
- 시간복잡도: **O(n³)**

#### 2.4 로그 반복

```
for (int i = 1; i < n; i *= 2) {
    // O(1)
}
```

- 반복 횟수: $\log_2 n$
- 시간복잡도: **O(log n)**

#### 2.5 로그 × 선형

```
for (int i = 1; i < n; i *= 2) {
    for (int j = 0; j < n; j++) {
        // O(1)
    }
}
```

- 바깥 루프: $O(\log n)$, 안쪽 루프: $O(n)$
- 시간복잡도: **O(n log n)**

### 3. 공간복잡도(Space Complexity)란?

**공간복잡도**는 알고리즘이 실행되는 동안 **필요한 메모리 공간의 크기**를 의미한다.

#### 📌 구성 요소

- 입력 공간: 입력 자체가 차지하는 공간
- 보조 공간: 추가로 사용하는 변수, 배열, 재귀 스택 등
- 전체 공간복잡도 = 입력 + 보조 공간

### 4. 공간복잡도 계산 예시

#### 4.1 고정 배열

```
int arr[100];
```

- 공간복잡도: **O(1)** (입력과 무관)

#### 4.2 입력 크기 기반 배열

```
int arr[n];
```

- 공간복잡도: **O(n)**

#### 4.3 이차원 배열

```
int matrix[n][n];
```

- 공간복잡도: **O(n²)**

#### 4.4 재귀 함수 (콜 스택 사용)

```
int factorial(int n) {
    if (n == 1) return 1;
    return n * factorial(n - 1);
}
```

- 재귀 깊이: $n$
- 공간복잡도: **O(n)** (콜 스택)

### 5. 시간복잡도 및 공간복잡도 요약 표

| 알고리즘 유형      | 시간복잡도         | 공간복잡도  |
| ------------------ | ------------------ | ----------- |
| 단순 for 루프      | $O(n)$             | $O(1)$      |
| 중첩 루프          | $O(n^2)$           | $O(1)$      |
| 재귀 DFS           | $O(n)$             | $O(n)$      |
| 병합 정렬          | $O(n \log n)$      | $O(n)$      |
| 퀵 정렬 (in-place) | $O(n \log n)$ 평균 | $O(\log n)$ |
| 해시 테이블        | $O(1)$ 평균        | $O(n)$      |

### 6. 실전 팁

| 상황                        | 전략                                                    |
| --------------------------- | ------------------------------------------------------- |
| 중첩 반복문이 많을수록      | 곱셈형 $O(n^2), O(n^3)$ 계열 가능성 ↑                   |
| 이분탐색/분할정복 사용 시   | 로그 복잡도 또는 $O(n \log n)$ 가능성 ↑                 |
| 모든 입력 조합을 고려한다면 | $O(2^n), O(n!)$ 등 지수 복잡도 위험                     |
| 재귀가 있다면               | 스택 메모리까지 공간복잡도 고려                         |
| 정답이 숫자가 아닌 구조라면 | 추가 구조 (배열, 리스트, 맵 등)의 메모리 사용 고려 필요 |

### 7. 결론

- **시간복잡도는 연산 횟수**, **공간복잡도는 메모리 사용량**이다.
- 입력 크기에 따라 어떻게 성능이 변화하는지 **계산식을 세우고 분석**하는 것이 핵심이다.
- 빅오 표기법은 성능을 요약하는 언어이며, **핵심 반복 구조**와 **재귀 깊이**, **자료구조의 크기**에 주목해야 한다.

## 재귀(Recursion)의 이해

### 1. 개요

**재귀(Recursion)**란, **함수가 자기 자신을 다시 호출하여 문제를 해결하는 방식**이다.
 일반적으로 **작은 문제로 분할하여 처리**할 수 있는 경우에 유용하며, 수학적 귀납법과 구조가 유사하다.

> 📌 핵심 조건:
>
> - **기저 조건(Base Case)**: 재귀를 멈추는 조건
> - **재귀 단계(Recursive Case)**: 자기 자신을 다시 호출하는 부분

### 2. 재귀의 구조

```
void function() {
    if (종료 조건) return;        // 기저 조건
    function();                  // 재귀 호출
}
```

### 3. 재귀의 동작 방식

- **함수 호출 시** 스택 프레임이 생성되어 **매개변수와 지역 변수**를 저장
- **스택 구조(LIFO)**에 따라 마지막 호출부터 순차적으로 **되돌아가며 종료**
- 재귀는 **숨겨진 스택(Stack)을 사용한 반복**과 동일한 효과

### 4. 예제 1: 팩토리얼

```
int factorial(int n) {
    if (n == 0) return 1;             // 기저 조건
    return n * factorial(n - 1);      // 재귀 호출
}
```

호출 예시: `factorial(4)`

```
factorial(4)
= 4 * factorial(3)
= 4 * 3 * factorial(2)
= 4 * 3 * 2 * factorial(1)
= 4 * 3 * 2 * 1 * factorial(0)
= 4 * 3 * 2 * 1 * 1
= 24
```

### 5. 예제 2: 피보나치 수열

```
int fib(int n) {
    if (n <= 1) return n;
    return fib(n - 1) + fib(n - 2);
}
```

- `fib(5)`의 호출 트리는 **이진 트리 구조**를 갖는다.
- 시간복잡도는 $O(2^n)$: 동일한 계산을 반복함

👉 **중복 호출 제거**를 위해 **메모이제이션** 또는 반복 방식으로 최적화 필요

### 6. 예제 3: 문자열 뒤집기 (재귀 활용)

```
void reversePrint(char* str, int index) {
    if (str[index] == '\0') return;
    reversePrint(str, index + 1);
    printf("%c", str[index]);
}
```

### 7. 예제 4: DFS (깊이 우선 탐색)

```
void dfs(int v) {
    visited[v] = 1;
    for (int i = 0; i < N; i++) {
        if (graph[v][i] && !visited[i]) {
            dfs(i);  // 재귀적으로 탐색
        }
    }
}
```

> DFS는 **재귀적 탐색 구조**의 대표 예다.

### 8. 재귀의 시간 및 공간 복잡도

| 항목        | 설명                               |
| ----------- | ---------------------------------- |
| 시간 복잡도 | 호출 횟수 × 각 호출의 연산량       |
| 공간 복잡도 | **재귀 깊이만큼 스택 프레임 사용** |

> 📌 예: `factorial(n)` → 시간 $O(n)$, 공간 $O(n)$ (콜스택)

### 9. 재귀 vs 반복 비교

| 기준        | 재귀                                  | 반복                           |
| ----------- | ------------------------------------- | ------------------------------ |
| 코드        | 간결함, 논리적 구조 표현 쉬움         | 상대적으로 길지만 명확함       |
| 성능        | 스택 오버헤드 존재                    | 메모리 효율 좋음               |
| 사용 예     | 분할 정복, 백트래킹, 트리 탐색 등     | 반복 횟수가 명확할 때 사용     |
| 최적화 여부 | Tail Recursion 최적화 가능 (C는 불가) | 루프 언롤링 등으로 최적화 가능 |

### 10. 주의할 점

- **기저 조건을 반드시 설정**해야 하며, 그렇지 않으면 무한 루프 발생
- 재귀 깊이가 깊어질 경우 **스택 오버플로우** 발생 가능
- 반복으로 바꿀 수 있다면 **성능 관점에서는 반복이 더 유리**

### 11. 결론

- 재귀는 **논리적 분할 구조를 표현하기 좋은 방식**이며, 문제의 구조가 자기 유사할 때 가장 적합하다.
- C 언어에서는 **함수 호출이 스택에 쌓인다**는 점을 항상 고려해야 한다.
- 성능이 중요한 경우, **반복으로 변환하거나 메모이제이션 최적화**를 함께 고려하는 것이 좋다.

## 반복(Iteration)과 비교

### 1. 핵심 개념 비교

| 항목          | **재귀 (Recursion)**                   | **반복 (Iteration)**                           |
| ------------- | -------------------------------------- | ---------------------------------------------- |
| **정의**      | 함수가 자기 자신을 호출하는 방식       | 조건을 만족할 때까지 루프를 반복 수행          |
| **기본 구조** | 함수 호출, 기저 조건, 스택 사용        | `for`, `while`, `do-while` 등 반복문 구조 사용 |
| **종료 조건** | 기저 조건 (base case)                  | 조건식 (`i < n` 등)                            |
| **사용 구조** | 트리 순회, 분할 정복, 백트래킹, DFS 등 | 순차적 계산, 누적 합산, 카운팅 등              |

### 2. C 언어 예제 비교

#### 2.1 팩토리얼

- ✅ 재귀 방식

```
int factorial_rec(int n) {
    if (n == 0) return 1;
    return n * factorial_rec(n - 1);
}
```

- ✅ 반복 방식

```
int factorial_iter(int n) {
    int result = 1;
    for (int i = 1; i <= n; i++)
        result *= i;
    return result;
}
```

#### 2.2 피보나치 수열

- ❌ 비효율적 재귀 (중복 계산)

```
int fib_rec(int n) {
    if (n <= 1) return n;
    return fib_rec(n - 1) + fib_rec(n - 2);
}
```

- ✅ 효율적 반복

```
int fib_iter(int n) {
    int a = 0, b = 1, temp;
    for (int i = 2; i <= n; i++) {
        temp = a + b;
        a = b;
        b = temp;
    }
    return b;
}
```

### 3. 성능 비교

| 항목        | **재귀**                                    | **반복**                         |
| ----------- | ------------------------------------------- | -------------------------------- |
| 시간 복잡도 | 종종 동일하지만, **재귀 깊이만큼 오버헤드** | 루프 자체만 반복되므로 더 효율적 |
| 공간 복잡도 | **스택 프레임 추가 사용** (콜스택 누적)     | 일반적으로 **O(1)**              |
| 오버헤드    | 함수 호출마다 메모리 및 레지스터 설정 필요  | 거의 없음                        |
| 코드 간결성 | **직관적** (특히 분할 정복 문제)            | 상대적으로 장황하지만 효율적     |
| 종료 제어   | 실수 시 **무한 재귀 → 스택 오버플로우**     | 반복 조건으로 제어 쉬움          |

### 4. 언제 재귀를 쓰는가?

| 상황                                 | 설명                                     |
| ------------------------------------ | ---------------------------------------- |
| **문제가 자기 자신을 포함할 때**     | 예: 트리 순회, 하노이의 탑, 분할 정복    |
| **논리적으로 표현이 더 직관적일 때** | 코드가 명료해지고 논리 흐름이 자연스러움 |
| **백트래킹이나 상태 트리 탐색 시**   | DFS, 순열 생성, 조합 생성 등             |

### 5. 언제 반복을 쓰는가?

| 상황                         | 설명                                         |
| ---------------------------- | -------------------------------------------- |
| **단순 계산 반복이 많을 때** | 누적 합, 배열 순회, 정렬 루프 등             |
| **성능이 중요한 경우**       | 스택 메모리 절약, 반복 오버헤드 없음         |
| **입력 크기가 매우 클 때**   | 재귀 깊이가 커지면 스택 오버플로우 위험 있음 |

### 6. 꼬리 재귀 최적화 (Tail Recursion)

**꼬리 재귀(Tail Recursion)**란, 함수의 **마지막 동작이 자기 자신을 호출하는 것**이다.

```
int factorial_tail(int n, int acc) {
    if (n == 0) return acc;
    return factorial_tail(n - 1, acc * n);
}
```

> ✅ 이론적으로 반복문으로 변환 가능
>  ❌ C 언어는 **꼬리 재귀 최적화(TCO)**를 자동 지원하지 않음

### 7. 재귀 → 반복으로 변환

많은 재귀 알고리즘은 명시적인 스택을 이용해 반복형으로 변환 가능하다.

예: DFS (재귀)

```
void dfs(int v) {
    visited[v] = 1;
    for (int i = 0; i < N; i++)
        if (graph[v][i] && !visited[i])
            dfs(i);
}
```

→ 스택을 사용한 반복 DFS

```
void dfs_iter(int start) {
    Stack s;
    init(&s);
    push(&s, start);

    while (!isEmpty(&s)) {
        int v = pop(&s);
        if (!visited[v]) {
            visited[v] = 1;
            for (int i = N - 1; i >= 0; i--)  // 역순 삽입
                if (graph[v][i])
                    push(&s, i);
        }
    }
}
```

### 8. 결론

| 정리                                         | 내용                                        |
| -------------------------------------------- | ------------------------------------------- |
| ✅ 재귀                                       | 코드가 간결하고 트리, 분할 문제에서 유리    |
| ✅ 반복                                       | 실행 속도와 메모리 사용에서 일반적으로 유리 |
| 🔁 대부분의 재귀는 반복으로 변환 가능         |                                             |
| ⚠️ C 언어에서는 스택 오버플로우에 주의해야 함 |                                             |

# 2.2 정렬 알고리즘(Sorting)

## 버블 정렬(Bubble Sort)

### 1. 개요

**버블 정렬(Bubble Sort)**은 **인접한 두 데이터를 비교하고, 조건에 따라 자리를 바꾸는 방식**을 이용해 정렬하는 알고리즘이다.
 큰 값(또는 작은 값)이 반복적으로 끝으로 밀려나며 마치 **거품(Bubble)이 위로 올라가는 모양**과 비슷하다는 데서 이름이 붙었다.

> 📌 정렬 방식:
>
> - 오름차순: 앞의 값이 크면 교환
> - 내림차순: 앞의 값이 작으면 교환

### 2. 알고리즘 동작 방식

#### 예: `[5, 3, 8, 4, 2]` 정렬

- 1회전: `[3, 5, 4, 2, 8]`
- 2회전: `[3, 4, 2, 5, 8]`
- 3회전: `[3, 2, 4, 5, 8]`
- 4회전: `[2, 3, 4, 5, 8]` → 정렬 완료

→ 매회 큰 값이 맨 뒤로 이동함

### 3. C 언어 구현

```
#include <stdio.h>

void bubbleSort(int arr[], int n) {
    for (int i = 0; i < n - 1; i++) {
        int swapped = 0;  // 최적화용 플래그
        for (int j = 0; j < n - 1 - i; j++) {
            if (arr[j] > arr[j + 1]) {
                // 스왑
                int temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
                swapped = 1;
            }
        }
        // 정렬 완료 시 조기 종료
        if (swapped == 0) break;
    }
}
```

#### 출력 함수 예시

```
void printArray(int arr[], int n) {
    for (int i = 0; i < n; i++)
        printf("%d ", arr[i]);
    printf("\n");
}
```

### 4. 시간복잡도

| 경우             | 비교 횟수            | 시간복잡도 |
| ---------------- | -------------------- | ---------- |
| 최선 (정렬 완료) | $n - 1$              | **O(n)**   |
| 평균             | $\frac{n(n - 1)}{2}$ | **O(n²)**  |
| 최악 (역순 정렬) | $\frac{n(n - 1)}{2}$ | **O(n²)**  |

> ✅ 최선의 경우 조기 종료(스왑 없음)를 활용하면 성능 향상 가능

### 5. 공간복잡도

- 별도의 메모리를 사용하지 않음 → **O(1)** (제자리 정렬)

### 6. 안정 정렬 여부

- ✅ **안정 정렬(Stable Sort)**:
   동일한 값의 상대적 순서가 유지됨

### 7. 장단점 정리

| 항목   | 설명                                 |
| ------ | ------------------------------------ |
| ✅ 장점 | 구현이 매우 간단함, 코드 직관적      |
| ✅ 장점 | 안정 정렬이며, 거의 정렬된 경우 빠름 |
| ❌ 단점 | 평균 및 최악 시간복잡도가 $O(n^2)$   |
| ❌ 단점 | 요소 수가 많아지면 심각하게 느려짐   |

### 8. 버블 정렬 vs 다른 정렬

| 알고리즘  | 평균 시간복잡도 | 공간복잡도 | 안정성   | 특징                         |
| --------- | --------------- | ---------- | -------- | ---------------------------- |
| 버블 정렬 | O(n²)           | O(1)       | ✅ 안정   | 가장 단순, 비교 기반         |
| 삽입 정렬 | O(n²)           | O(1)       | ✅ 안정   | 거의 정렬된 배열에 효과적    |
| 선택 정렬 | O(n²)           | O(1)       | ❌ 불안정 | 교환 최소, 항상 일정 비교 수 |
| 병합 정렬 | O(n log n)      | O(n)       | ✅ 안정   | 외부 정렬/병합 필요          |
| 퀵 정렬   | O(n log n) 평균 | O(log n)   | ❌ 불안정 | 분할 정복 기반, 실전 최강    |

### 9. 실전 사용 여부

- ❌ 실전 사용은 거의 없음
- ✅ 교육/기초 알고리즘 학습용으로는 **최적의 도입용 알고리즘**

### 10. 결론

- 버블 정렬은 **교환 정렬의 대표적인 형태**이며, 알고리즘 학습 입문에 적합하다.
- 그러나 **실전 환경**에서는 성능이 낮기 때문에 **퀵 정렬**, **병합 정렬**, **힙 정렬** 등이 대체된다.
- **조기 종료 최적화**를 적용하면 부분 정렬된 데이터에는 비교적 빠르게 작동할 수 있다.

## 선택 정렬(Selection Sort)

### 1. 개요

**선택 정렬(Selection Sort)**은 **전체 데이터 중 가장 작은(또는 큰) 값을 찾아서 맨 앞자리와 교환**하는 방식으로 정렬하는 알고리즘이다.
 정렬이 완료될 때까지 이러한 과정을 반복한다.

> 📌 핵심 동작:
>
> - i번째 위치에 i번째로 작은 값을 찾아 배치
> - 하나의 자리를 정하고, 그 자리에 들어갈 **최적값을 선택**

### 2. 작동 방식 (오름차순 기준)

#### 예: `[5, 3, 8, 4, 2]`

- 1회전: `2`와 `5` 교환 → `[2, 3, 8, 4, 5]`
- 2회전: `3`은 제자리 → `[2, 3, 8, 4, 5]`
- 3회전: `4`와 `8` 교환 → `[2, 3, 4, 8, 5]`
- 4회전: `5`와 `8` 교환 → `[2, 3, 4, 5, 8]`
- 5회전: 종료

### 3. C 언어 구현

```
#include <stdio.h>

void selectionSort(int arr[], int n) {
    for (int i = 0; i < n - 1; i++) {
        int min_idx = i;

        for (int j = i + 1; j < n; j++) {
            if (arr[j] < arr[min_idx])
                min_idx = j;
        }

        // 현재 i번째와 최소값 교환
        if (min_idx != i) {
            int temp = arr[i];
            arr[i] = arr[min_idx];
            arr[min_idx] = temp;
        }
    }
}
```

#### 배열 출력 함수

```
void printArray(int arr[], int n) {
    for (int i = 0; i < n; i++)
        printf("%d ", arr[i]);
    printf("\n");
}
```

### 4. 시간복잡도

| 경우 | 비교 횟수               | 시간복잡도 |
| ---- | ----------------------- | ---------- |
| 최선 | 항상 $\frac{n(n-1)}{2}$ | **O(n²)**  |
| 평균 | $\frac{n(n-1)}{2}$      | **O(n²)**  |
| 최악 | $\frac{n(n-1)}{2}$      | **O(n²)**  |

> 비교는 항상 일정하지만, 교환 횟수는 최대 $n-1$번으로 **버블 정렬보다 교환은 적다**.

### 5. 공간복잡도

- **O(1)**
   → 배열 내에서 직접 교환하는 **제자리 정렬(in-place)**

### 6. 안정성

- ❌ **불안정 정렬**
   동일한 값의 상대적 순서가 바뀔 수 있음

> 예: `[5_A, 5_B, 2]` → 선택 정렬 → `[2, 5_B, 5_A]` (순서 바뀜)

### 7. 장단점 정리

| 항목   | 설명                                    |
| ------ | --------------------------------------- |
| ✅ 장점 | 구현이 간단하고 직관적임                |
| ✅ 장점 | 교환 횟수가 적음 (최대 $n-1$번)         |
| ❌ 단점 | 비교 횟수는 항상 많음 → 성능 저하       |
| ❌ 단점 | 안정 정렬이 아님, 실제 사용은 거의 없음 |

### 8. 다른 정렬 알고리즘과 비교

| 알고리즘  | 평균 시간복잡도 | 공간복잡도 | 안정성   | 특징                                      |
| --------- | --------------- | ---------- | -------- | ----------------------------------------- |
| 선택 정렬 | O(n²)           | O(1)       | ❌ 불안정 | 비교 횟수 일정, 교환은 적음               |
| 버블 정렬 | O(n²)           | O(1)       | ✅ 안정   | 비교 많고 교환도 많음                     |
| 삽입 정렬 | O(n²)           | O(1)       | ✅ 안정   | 거의 정렬된 경우 매우 빠름                |
| 퀵 정렬   | O(n log n) 평균 | O(log n)   | ❌ 불안정 | 실전 최강 성능, 평균은 빠르나 최악은 느림 |
| 병합 정렬 | O(n log n)      | O(n)       | ✅ 안정   | 성능 보장되며, 외부 정렬에 적합           |

### 9. 실전 사용 여부

- ❌ **대규모 데이터에는 부적합**
- ✅ 알고리즘 학습, 작은 데이터, 제한된 환경에서 사용

### 10. 결론

- 선택 정렬은 **가장 직관적이고 간단한 정렬 알고리즘 중 하나**
- 하지만 **비효율적인 시간복잡도** 때문에 실전에서는 거의 사용되지 않음
- 학습 용도로는 **정렬의 동작 원리 이해에 매우 유용**

## 삽입 정렬(Insertion Sort)

### 1. 개요

**삽입 정렬(Insertion Sort)**은 배열의 앞쪽부터 차례대로 **정렬된 부분 배열을 유지하면서**, 현재 요소를 **올바른 위치에 "삽입"**하는 방식이다.
 정렬된 배열에 새로운 값을 끼워 넣는다는 개념에서 삽입 정렬이라고 한다.

> 📌 인간이 **카드 게임에서 손에 카드를 정렬하는 방식**과 매우 유사한 알고리즘이다.

### 2. 작동 방식 (오름차순 기준)

#### 예: `[5, 3, 8, 4, 2]`

- `3`을 `5` 앞에 삽입 → `[3, 5, 8, 4, 2]`
- `8`은 제자리 유지 → `[3, 5, 8, 4, 2]`
- `4`를 `5`, `8` 앞에 삽입 → `[3, 4, 5, 8, 2]`
- `2`를 맨 앞으로 삽입 → `[2, 3, 4, 5, 8]`

### 3. C 언어 구현

```
#include <stdio.h>

void insertionSort(int arr[], int n) {
    for (int i = 1; i < n; i++) {
        int key = arr[i];
        int j = i - 1;

        // key보다 큰 요소들은 오른쪽으로 한 칸 이동
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j--;
        }

        // key를 올바른 위치에 삽입
        arr[j + 1] = key;
    }
}
```

#### 배열 출력 함수

```
void printArray(int arr[], int n) {
    for (int i = 0; i < n; i++)
        printf("%d ", arr[i]);
    printf("\n");
}
```

### 4. 시간복잡도

| 경우               | 비교 횟수            | 시간복잡도 |
| ------------------ | -------------------- | ---------- |
| 최선 (정렬된 경우) | $n - 1$              | **O(n)**   |
| 평균               | 약 $\frac{n^2}{4}$   | **O(n²)**  |
| 최악 (역순 정렬)   | $\frac{n(n - 1)}{2}$ | **O(n²)**  |

> ✅ 거의 정렬된 배열에 대해서는 매우 빠름

### 5. 공간복잡도

- **O(1)**
   → 제자리 정렬 (In-place Sort)

### 6. 안정 정렬 여부

- ✅ **안정 정렬(Stable Sort)**
   동일한 값의 **기존 순서를 유지**

> 예: `[5_A, 5_B, 3]` → `[3, 5_A, 5_B]`

### 7. 장단점 요약

| 항목   | 설명                                                |
| ------ | --------------------------------------------------- |
| ✅ 장점 | 거의 정렬된 경우 빠름, 구현 간단함                  |
| ✅ 장점 | 안정 정렬, in-place 정렬로 추가 메모리 필요 없음    |
| ❌ 단점 | 역정렬/무작위 데이터에 대해 성능 매우 낮음 $O(n^2)$ |

### 8. 삽입 정렬 vs 다른 정렬 비교

| 알고리즘  | 평균 시간복잡도 | 공간복잡도 | 안정성   | 특징                           |
| --------- | --------------- | ---------- | -------- | ------------------------------ |
| 삽입 정렬 | O(n²)           | O(1)       | ✅ 안정   | 거의 정렬된 배열에 매우 유리   |
| 버블 정렬 | O(n²)           | O(1)       | ✅ 안정   | 단순하지만 교환 횟수 많음      |
| 선택 정렬 | O(n²)           | O(1)       | ❌ 불안정 | 교환 적음, 비교는 많음         |
| 병합 정렬 | O(n log n)      | O(n)       | ✅ 안정   | 외부 정렬에 적합               |
| 퀵 정렬   | O(n log n) 평균 | O(log n)   | ❌ 불안정 | 실전 성능 최고, 부분 정렬 불가 |

### 9. 삽입 정렬이 유리한 경우

| 상황                      | 이유                                  |
| ------------------------- | ------------------------------------- |
| 거의 정렬된 데이터        | 한두 번의 비교로 끝남 → O(n)에 가까움 |
| 작은 배열 (n < 20~30)     | 다른 정렬보다 오히려 빠를 수 있음     |
| 정렬 안정성이 중요한 경우 | 동일한 값의 순서를 보존해야 하는 경우 |

### 10. 결론

- 삽입 정렬은 **단순하고 안정적인 정렬 알고리즘**으로, 알고리즘 학습에 매우 적합하다.
- **데이터가 거의 정렬되어 있거나, 작은 입력 크기에서는 효율적**이지만,
   **입력 크기가 커지면 $O(n^2)$**로 인해 실전 성능은 좋지 않다.
- 다른 고급 정렬 알고리즘들과 함께 쓰여 **하이브리드 정렬 알고리즘의 일부**로도 활용된다 (예: Timsort의 insertion sort 구간 등).

## 병합 정렬(Merge Sort)

### 1. 개요

**병합 정렬(Merge Sort)**은 배열을 **반으로 나누고**, 각각을 **재귀적으로 정렬한 뒤**,
 두 개의 정렬된 배열을 **병합(Merge)**하여 전체를 정렬하는 알고리즘이다.

> 📌 핵심 전략:
>
> - **Divide**: 배열을 반으로 쪼갠다
> - **Conquer**: 각각 재귀적으로 정렬
> - **Combine**: 두 배열을 병합하며 정렬

### 2. 작동 방식 예 (오름차순)

배열: `[8, 4, 5, 2, 9, 1, 6, 3]`

```
[8, 4, 5, 2]   [9, 1, 6, 3]   ← Divide
[8, 4] [5, 2]  [9, 1] [6, 3]
[4, 8] [2, 5]  [1, 9] [3, 6]   ← Conquer (정렬)
[2, 4, 5, 8]   [1, 3, 6, 9]
[1, 2, 3, 4, 5, 6, 8, 9]       ← Merge
```

### 3. C 언어 구현

```
#include <stdio.h>

void merge(int arr[], int left, int mid, int right) {
    int n1 = mid - left + 1;
    int n2 = right - mid;

    int L[n1], R[n2];  // 임시 배열

    // L[], R[]에 분리
    for (int i = 0; i < n1; i++)
        L[i] = arr[left + i];
    for (int j = 0; j < n2; j++)
        R[j] = arr[mid + 1 + j];

    // 병합
    int i = 0, j = 0, k = left;
    while (i < n1 && j < n2) {
        arr[k++] = (L[i] <= R[j]) ? L[i++] : R[j++];
    }

    // 남은 요소 복사
    while (i < n1)
        arr[k++] = L[i++];
    while (j < n2)
        arr[k++] = R[j++];
}

void mergeSort(int arr[], int left, int right) {
    if (left < right) {
        int mid = (left + right) / 2;
        mergeSort(arr, left, mid);        // 좌측 절반 정렬
        mergeSort(arr, mid + 1, right);   // 우측 절반 정렬
        merge(arr, left, mid, right);     // 병합
    }
}
```

#### 출력 함수

```
void printArray(int arr[], int n) {
    for (int i = 0; i < n; i++)
        printf("%d ", arr[i]);
    printf("\n");
}
```

### 4. 시간 복잡도

| 단계            | 복잡도 설명                     | 총합        |
| --------------- | ------------------------------- | ----------- |
| 분할 횟수       | 배열을 $\log_2 n$ 단계까지 나눔 | $\log n$    |
| 병합 시간       | 각 단계마다 전체 배열을 스캔    | $n$         |
| 전체 시간복잡도 | $O(n \log n)$                   | ✅ 항상 동일 |

> ⚠️ **최선/최악/평균 모두 동일**한 성능을 보장함

### 5. 공간 복잡도

- **O(n)** → 병합 시 **임시 배열을 사용**하기 때문
- 추가 메모리를 사용하지 않으면 **제자리 병합 정렬**(in-place merge sort) 필요 (복잡도 상승)

### 6. 안정 정렬 여부

- ✅ **안정 정렬 (Stable Sort)**
   동일한 값을 가진 요소들의 **상대적 순서가 유지됨**

### 7. 장단점 정리

| 항목   | 설명                                           |
| ------ | ---------------------------------------------- |
| ✅ 장점 | 최악의 경우에도 $O(n \log n)$, 안정 정렬       |
| ✅ 장점 | 큰 입력에도 예측 가능한 성능, 외부 정렬에 유리 |
| ❌ 단점 | **O(n)**의 추가 메모리 필요 (in-place 아님)    |
| ❌ 단점 | 코드 구현이 단순 정렬보다 복잡함               |

### 8. 병합 정렬 vs 다른 정렬

| 알고리즘  | 평균 시간복잡도 | 공간복잡도 | 안정성   | 특징                                            |
| --------- | --------------- | ---------- | -------- | ----------------------------------------------- |
| 병합 정렬 | O(n log n)      | O(n)       | ✅ 안정   | 항상 빠르고 안정적, 외부 정렬에 최적            |
| 퀵 정렬   | O(n log n) 평균 | O(log n)   | ❌ 불안정 | 내부 정렬 최고 속도, 최악 O(n²), 재귀 기반      |
| 힙 정렬   | O(n log n)      | O(1)       | ❌ 불안정 | 메모리 사용 적고 빠름, 힙 자료구조 활용         |
| 삽입 정렬 | O(n²)           | O(1)       | ✅ 안정   | 거의 정렬된 데이터에는 효과적, 작은 배열에 유리 |

### 9. 실전 사용

| 분야                   | 이유                                       |
| ---------------------- | ------------------------------------------ |
| **외부 정렬**          | 디스크/파일 기반의 정렬 (RAM 부족 시 사용) |
| **데이터베이스 정렬**  | 안정성이 보장되어야 하는 경우              |
| **병렬 정렬 알고리즘** | Merge 단계에서 병렬 처리 가능              |

### 10. 결론

- 병합 정렬은 **가장 안정적이며 예측 가능한 시간 복잡도**를 갖는 정렬 알고리즘이다.
- 다만 **추가 메모리 공간이 필요**하고, **재귀 기반의 복잡한 구현**이 단점일 수 있다.
- 실전에서는 병합 정렬이 **외부 정렬**, **안정 정렬이 필요한 환경**, **하이브리드 정렬(Timsort)** 등에 자주 응용된다.

## 퀵 정렬(Quick Sort)

### 1. 개요

**퀵 정렬(Quick Sort)**은 배열에서 **기준(pivot)을 하나 정해**, 그 기준보다 작은 값은 왼쪽, 큰 값은 오른쪽으로 보내며 분할을 반복하는 정렬 알고리즘이다.
 병합 정렬처럼 분할 정복 방식을 따르되, **정렬은 분할하면서 처리**한다.

> 📌 퀵 정렬의 핵심 개념:
>
> - **분할(Partition)**: 피벗을 기준으로 작은/큰 요소 분리
> - **정복(Conquer)**: 각각의 파티션에 재귀적으로 퀵 정렬 수행

### 2. 작동 방식 예 (오름차순 기준)

배열: `[8, 4, 7, 2, 9, 1]`
 피벗: `8` 선택 시

1. `[4, 7, 2, 1]` | `8` | `[9]` 으로 분할
2. 각 부분 재귀 정렬
    → 최종 결과: `[1, 2, 4, 7, 8, 9]`

### 3. C 언어 구현

```
#include <stdio.h>

// 값 교환 함수
void swap(int* a, int* b) {
    int tmp = *a;
    *a = *b;
    *b = tmp;
}

// 파티션 함수: 피벗 기준 분할
int partition(int arr[], int low, int high) {
    int pivot = arr[high];  // 마지막 요소를 피벗으로 설정
    int i = low - 1;         // 작은 요소의 인덱스

    for (int j = low; j < high; j++) {
        if (arr[j] <= pivot) {
            i++;
            swap(&arr[i], &arr[j]);
        }
    }

    swap(&arr[i + 1], &arr[high]);  // 피벗을 올바른 위치로
    return i + 1;
}

// 퀵 정렬 함수
void quickSort(int arr[], int low, int high) {
    if (low < high) {
        int pi = partition(arr, low, high);  // 파티션 인덱스
        quickSort(arr, low, pi - 1);         // 왼쪽 정렬
        quickSort(arr, pi + 1, high);        // 오른쪽 정렬
    }
}
```

#### 배열 출력 함수

```
void printArray(int arr[], int n) {
    for (int i = 0; i < n; i++)
        printf("%d ", arr[i]);
    printf("\n");
}
```

### 4. 시간 복잡도

| 경우             | 시간 복잡도   |
| ---------------- | ------------- |
| 최선 (균등 분할) | $O(n \log n)$ |
| 평균             | $O(n \log n)$ |
| 최악 (편향 분할) | $O(n^2)$      |

> ⚠️ 피벗 선택이 불균형할 경우 → 편향된 분할 발생
>  → 이를 피하기 위해 **중앙값 선택**, **랜덤 피벗**, **Hoare 방식** 등을 사용

### 5. 공간 복잡도

- **$O(\log n)$** (재귀 스택 사용)
- 제자리 정렬 (in-place) → 추가 배열 없음

### 6. 안정성

- ❌ **불안정 정렬 (Unstable Sort)**
   동일한 값의 **상대적 순서가 바뀔 수 있음**

### 7. 장단점 정리

| 항목   | 설명                                                    |
| ------ | ------------------------------------------------------- |
| ✅ 장점 | 평균적으로 매우 빠름 (실무에서 가장 널리 사용되는 정렬) |
| ✅ 장점 | in-place 정렬 (추가 메모리 거의 사용 안 함)             |
| ❌ 단점 | 최악의 경우 성능 저하 ($O(n^2)$)                        |
| ❌ 단점 | 불안정 정렬                                             |

### 8. 퀵 정렬 vs 병합 정렬 vs 힙 정렬

| 정렬 알고리즘 | 평균 시간  | 최악 시간  | 공간복잡도 | 안정성   | 특징                             |
| ------------- | ---------- | ---------- | ---------- | -------- | -------------------------------- |
| **퀵 정렬**   | O(n log n) | O(n²)      | O(log n)   | ❌ 불안정 | 실전 성능 최강, in-place 정렬    |
| **병합 정렬** | O(n log n) | O(n log n) | O(n)       | ✅ 안정   | 외부 정렬, 데이터베이스용        |
| **힙 정렬**   | O(n log n) | O(n log n) | O(1)       | ❌ 불안정 | 공간 효율 높음, 이론적 성능 우수 |

### 9. 퀵 정렬 피벗 선택 전략

| 방식                         | 설명                                         |
| ---------------------------- | -------------------------------------------- |
| **마지막 요소**              | 가장 간단하지만 최악의 경우 발생 가능        |
| **첫 번째 요소**             | 동일하게 편향 발생 위험 존재                 |
| **중앙값 (Median-of-three)** | 첫/중간/끝 중 중간값 선택 → 성능 향상 가능   |
| **랜덤 피벗**                | 평균 성능 유지에 도움 → 실전에서 널리 사용됨 |

### 10. 결론

- 퀵 정렬은 **내부 정렬 중 가장 빠른 평균 성능**을 제공한다.
- 구현이 간단하며, **추가 메모리 없이 작동**하는 제자리 정렬이다.
- **최악의 경우 성능 저하를 방지**하기 위해 피벗 선택 전략이 매우 중요하다.
- 실무에서는 대부분 **하이브리드 정렬(Timsort, IntroSort)**에서 핵심 컴포넌트로 활용된다.

## 힙 정렬(Heap Sort)

### 1. 개요

**힙 정렬(Heap Sort)**은 **완전 이진 트리 기반의 힙 자료구조**를 이용하여
 **최대값 또는 최소값을 반복적으로 꺼내어 정렬**하는 알고리즘이다.

> 📌 핵심 개념:
>
> - 힙(Heap)은 **완전 이진 트리 기반의 우선순위 큐**
> - **최대 힙 (Max-Heap)**: 부모 ≥ 자식
> - **최소 힙 (Min-Heap)**: 부모 ≤ 자식
> - 힙 정렬은 일반적으로 **최대 힙**을 사용하여 **오름차순 정렬**을 수행

### 2. 힙 정렬 동작 원리

1. **입력 배열을 최대 힙으로 구성**
2. 루트(최댓값)를 끝으로 보내고, **힙을 재구성**
3. 위 과정을 배열이 비워질 때까지 반복

### 3. 힙 정렬: C 언어 구현

```
#include <stdio.h>

// 두 값을 교환
void swap(int* a, int* b) {
    int tmp = *a;
    *a = *b;
    *b = tmp;
}

// 최대 힙으로 만드는 함수 (heapify)
void heapify(int arr[], int n, int i) {
    int largest = i;           // 루트
    int left = 2 * i + 1;      // 왼쪽 자식
    int right = 2 * i + 2;     // 오른쪽 자식

    // 왼쪽 자식이 루트보다 크면
    if (left < n && arr[left] > arr[largest])
        largest = left;

    // 오른쪽 자식이 현재 최대값보다 크면
    if (right < n && arr[right] > arr[largest])
        largest = right;

    // 가장 큰 값이 루트가 아니라면 교환
    if (largest != i) {
        swap(&arr[i], &arr[largest]);
        heapify(arr, n, largest);  // 재귀 호출
    }
}

// 힙 정렬 메인 함수
void heapSort(int arr[], int n) {
    // 1. 최대 힙 구성
    for (int i = n / 2 - 1; i >= 0; i--)
        heapify(arr, n, i);

    // 2. 루트 제거 후 재정렬
    for (int i = n - 1; i > 0; i--) {
        swap(&arr[0], &arr[i]);     // 최대값 → 뒤로
        heapify(arr, i, 0);         // 루트 재정렬
    }
}
```

#### 배열 출력 함수

```
void printArray(int arr[], int n) {
    for (int i = 0; i < n; i++)
        printf("%d ", arr[i]);
    printf("\n");
}
```

### 4. 시간 복잡도

| 단계                          | 복잡도         |
| ----------------------------- | -------------- |
| 힙 구성                       | $O(n)$         |
| 정렬 반복 (heapify 호출 포함) | $O(n \log n)$  |
| **총합**                      | **O(n log n)** |

> 모든 경우(최선/평균/최악)에 **일관된 성능** 제공

### 5. 공간 복잡도

- **O(1)** → **제자리 정렬 (In-place Sort)**
- 별도의 추가 배열 없음

### 6. 안정성

- ❌ **불안정 정렬(Unstable Sort)**
   동일한 값의 순서가 바뀔 수 있음

### 7. 힙 정렬 예시

```
입력: [4, 10, 3, 5, 1]

1. 최대 힙 구성: [10, 5, 3, 4, 1]
2. 최대값 10 제거 → [5, 4, 3, 1, 10]
3. 최대값 5 제거  → [4, 1, 3, 5, 10]
4. ...
결과: [1, 3, 4, 5, 10]
```

### 8. 장단점 비교

| 항목   | 설명                                                 |
| ------ | ---------------------------------------------------- |
| ✅ 장점 | **O(n log n)** 일관된 성능                           |
| ✅ 장점 | 제자리 정렬, 공간 효율성 뛰어남                      |
| ❌ 단점 | 안정 정렬이 아님, 상대적으로 구현 복잡함             |
| ❌ 단점 | 실제 데이터에서 퀵 정렬보다 평균 속도는 느릴 수 있음 |

### 9. 다른 정렬과의 비교

| 알고리즘  | 평균 시간복잡도 | 공간복잡도 | 안정성   | 특징                            |
| --------- | --------------- | ---------- | -------- | ------------------------------- |
| 힙 정렬   | O(n log n)      | O(1)       | ❌ 불안정 | 균등 성능, 메모리 적게 씀       |
| 퀵 정렬   | O(n log n) 평균 | O(log n)   | ❌ 불안정 | 실전 성능 최강, 최악 O(n²) 가능 |
| 병합 정렬 | O(n log n)      | O(n)       | ✅ 안정   | 안정성, 외부 정렬 적합          |

### 10. 결론

- 힙 정렬은 **균일한 시간복잡도 O(n log n)**을 갖는 효율적인 정렬 알고리즘이다.
- **추가 메모리가 거의 필요 없고**, 항상 일관된 성능을 유지하며 **실시간 시스템이나 임베디드 환경에서 유용**하다.
- 다만, **불안정 정렬**이고, 평균 속도는 퀵 정렬보다 느릴 수 있다.

## 기수 정렬(Radix Sort), 계수 정렬(Counting Sort)

### 🟩 1. 기수 정렬 (Radix Sort)

#### 📌 정의

**기수 정렬(Radix Sort)**은 숫자를 **자릿수 기준으로 반복적으로 정렬**하는 알고리즘이다.
 가장 작은 자릿수(1의 자리)부터 시작해 큰 자리까지 정렬해나가며 **안정적인 정렬 방식**을 반복 적용한다.

> 💡 **비교 기반이 아님**
>
> - 정렬 대상: 자연수/정수/문자열(고정 길이)
> - 보조 정렬: **안정 정렬(보통 Counting Sort)** 필요

#### 🔧 C 코드 예시

```
#include <stdio.h>

int getMax(int arr[], int n) {
    int max = arr[0];
    for (int i = 1; i < n; i++)
        if (arr[i] > max) max = arr[i];
    return max;
}

void countingSortByDigit(int arr[], int n, int exp) {
    int output[n];
    int count[10] = {0};

    // 현재 자릿수의 빈도 계산
    for (int i = 0; i < n; i++)
        count[(arr[i] / exp) % 10]++;

    // 누적 합계 → 정렬 위치 계산
    for (int i = 1; i < 10; i++)
        count[i] += count[i - 1];

    // output에 정렬된 값 삽입 (뒤에서부터 처리 = 안정 정렬)
    for (int i = n - 1; i >= 0; i--) {
        int digit = (arr[i] / exp) % 10;
        output[--count[digit]] = arr[i];
    }

    // 결과 복사
    for (int i = 0; i < n; i++)
        arr[i] = output[i];
}

void radixSort(int arr[], int n) {
    int max = getMax(arr, n);

    // 각 자릿수마다 Counting Sort 적용
    for (int exp = 1; max / exp > 0; exp *= 10)
        countingSortByDigit(arr, n, exp);
}
```

#### 📈 성능 분석

| 항목             | 설명                                           |
| ---------------- | ---------------------------------------------- |
| 시간 복잡도      | **O(d × (n + k))**  → d: 자릿수, k: 기수 (0~9) |
| 공간 복잡도      | O(n + k)                                       |
| 정렬 안정성      | ✅ **안정 정렬**                                |
| 제자리 정렬 여부 | ❌ 추가 배열 필요                               |

#### ✅ 장점 / ❌ 단점

| ✅ 장점                                | ❌ 단점                                   |
| ------------------------------------- | ---------------------------------------- |
| 비교 기반이 아님 → **정렬 경계 초월** | **정수 또는 고정 길이 문자열에 한정**    |
| 빠른 시간복잡도 O(n) 가능성 있음      | 자리수마다 Counting Sort → 메모리 사용 ↑ |

### 🟨 2. 계수 정렬 (Counting Sort)

#### 📌 정의

**계수 정렬(Counting Sort)**은 **값 자체를 인덱스로 생각하여 빈도수를 세고**, 이를 기반으로 정렬하는 알고리즘이다.
 **최댓값이 작고 정수가 양수일 때 유리**하며, 자릿수 정렬이 아닌 **값 자체를 직접 다룬다.**

#### 🔧 C 코드 예시

```
#include <stdio.h>
#include <string.h>

void countingSort(int arr[], int n) {
    int max = arr[0];
    for (int i = 1; i < n; i++)
        if (arr[i] > max) max = arr[i];

    int count[max + 1];
    memset(count, 0, sizeof(count));

    // 빈도수 계산
    for (int i = 0; i < n; i++)
        count[arr[i]]++;

    // 누적 합계 계산
    for (int i = 1; i <= max; i++)
        count[i] += count[i - 1];

    // 정렬 결과 배열
    int output[n];
    for (int i = n - 1; i >= 0; i--) {
        output[--count[arr[i]]] = arr[i];
    }

    // 결과 복사
    for (int i = 0; i < n; i++)
        arr[i] = output[i];
}
```

#### 📈 성능 분석

| 항목             | 설명                                         |
| ---------------- | -------------------------------------------- |
| 시간 복잡도      | **O(n + k)**  → k: 값의 범위 (max - min + 1) |
| 공간 복잡도      | O(n + k)                                     |
| 정렬 안정성      | ✅ **안정 정렬**                              |
| 제자리 정렬 여부 | ❌ 추가 배열 필요                             |

#### ✅ 장점 / ❌ 단점

| ✅ 장점                            | ❌ 단점                                           |
| --------------------------------- | ------------------------------------------------ |
| 매우 빠름 → 선형 시간 복잡도 가능 | **범위가 넓거나 음수가 있으면 부적합**           |
| 비교 없이 값 자체로 정렬 가능     | **메모리 낭비**: count 배열이 값 범위만큼 필요함 |
| 정수 기반, 카운팅만으로 정렬 가능 | 문자열/부동소수점 등 일반적인 값에는 부적합      |

### 🟦 Radix vs Counting 정리

| 항목        | **Radix Sort**                         | **Counting Sort**                      |
| ----------- | -------------------------------------- | -------------------------------------- |
| 대상        | 다자리 정수, 고정 길이 문자열          | 작은 정수 (ex. 0~1000)                 |
| 핵심 기준   | **자릿수마다 Counting Sort 적용**      | **전체 값의 범위를 Count 배열로 계산** |
| 복잡도      | O(d(n + k))                            | O(n + k)                               |
| 안정성      | ✅ 안정 정렬                            | ✅ 안정 정렬                            |
| 공간 사용량 | O(n + k)                               | O(n + k)                               |
| 한계        | 정수에만 적용 가능, 자릿수 많으면 느림 | 큰 값이나 음수 있을 경우 메모리 낭비   |

### ✅ 결론

- **Radix Sort**: 정수 자릿수를 기준으로 정렬 → **큰 값도 정렬 가능**, **안정성 유지**, 자릿수 d가 작을수록 유리
- **Counting Sort**: 값 범위가 작을 때 최적 → **비교 없이 매우 빠름**, 단점은 **범위 제한과 메모리**

### 다음 주제

- **Radix Sort로 문자열 정렬**
- **Counting Sort를 음수까지 확장하는 방법**
- **Counting Sort를 이용한 Histogram 구현**

## 안정성, 제자리성, 비교 기반/비비교 기반 정렬

> **안정성**, **제자리성**, **비교 기반/비비교 기반 정렬**의 개념과 차이점 정리

### 🟦 1. 정렬의 안정성 (Stability)

#### 📌 정의

정렬 알고리즘이 **동일한 키 값을 가진 원소들의 상대적 순서를 유지하는가**를 의미한다.

> 예시 입력:
>  `[5_A, 3, 5_B, 2]` (5_A와 5_B는 다른 객체이지만 값은 같음)
>
> - **안정 정렬** → `[2, 3, 5_A, 5_B]`
> - **불안정 정렬** → `[2, 3, 5_B, 5_A]`

#### ✅ 안정 정렬이 필요한 경우

- **복합 정렬(2차 기준)**
   예: “나이 오름차순 정렬 후, 이름 사전순 정렬”
- **데이터 가공 과정에서 원본 순서 유지가 중요할 때**

#### 주요 알고리즘의 안정성

| 알고리즘    | 안정성   |
| ----------- | -------- |
| 버블 정렬   | ✅ 안정   |
| 삽입 정렬   | ✅ 안정   |
| 병합 정렬   | ✅ 안정   |
| 카운팅 정렬 | ✅ 안정   |
| 기수 정렬   | ✅ 안정   |
| 선택 정렬   | ❌ 불안정 |
| 퀵 정렬     | ❌ 불안정 |
| 힙 정렬     | ❌ 불안정 |

### 🟨 2. 제자리 정렬 (In-place Sort)

#### 📌 정의

정렬 과정에서 **추가적인 메모리를 거의 사용하지 않는 정렬**을 말한다.
 보통은 **O(1)** 또는 **O(log n)** 수준의 **스택/포인터**만을 사용하는 것이 기준이다.

#### 예시

| 알고리즘    | 제자리 정렬 여부     | 추가 메모리 |
| ----------- | -------------------- | ----------- |
| 버블 정렬   | ✅ 예                 | O(1)        |
| 선택 정렬   | ✅ 예                 | O(1)        |
| 삽입 정렬   | ✅ 예                 | O(1)        |
| 퀵 정렬     | ✅ 예 (스택 O(log n)) | O(log n)    |
| 힙 정렬     | ✅ 예                 | O(1)        |
| 병합 정렬   | ❌ 아니오             | O(n)        |
| 카운팅 정렬 | ❌ 아니오             | O(k + n)    |
| 기수 정렬   | ❌ 아니오             | O(n + k)    |

#### ✅ 제자리 정렬이 필요한 경우

- 메모리가 제한된 환경 (임베디드 시스템, 대형 데이터)
- 실시간 정렬 작업 (인플레이스로 처리)

### 🟥 3. 비교 기반 vs 비비교 기반 정렬

#### 📌 비교 기반 정렬 (Comparison-Based Sort)

- 정렬의 핵심 기준: **두 값을 비교 → 크기 판단**
- 이론적 **하한 시간복잡도: O(n log n)**
- 모든 일반적 데이터형에 적용 가능

#### 대표 알고리즘

| 알고리즘  | 시간복잡도 평균 | 안정성   | 제자리성 |
| --------- | --------------- | -------- | -------- |
| 버블 정렬 | O(n²)           | ✅ 안정   | ✅ 예     |
| 삽입 정렬 | O(n²)           | ✅ 안정   | ✅ 예     |
| 선택 정렬 | O(n²)           | ❌ 불안정 | ✅ 예     |
| 병합 정렬 | O(n log n)      | ✅ 안정   | ❌ 아니오 |
| 퀵 정렬   | O(n log n)      | ❌ 불안정 | ✅ 예     |
| 힙 정렬   | O(n log n)      | ❌ 불안정 | ✅ 예     |

#### 📌 비비교 기반 정렬 (Non-Comparison Sort)

- 정렬의 기준: **값의 자릿수, 범위 등 구조적 특성 활용**
- 이론적 하한 없음 → **O(n)** 가능
- 정수, 문자열처럼 **구조가 정의된 데이터**에만 사용 가능

#### 대표 알고리즘

| 알고리즘    | 시간복잡도 평균  | 안정성      | 제자리성      |
| ----------- | ---------------- | ----------- | ------------- |
| 카운팅 정렬 | O(n + k)         | ✅ 안정      | ❌ 아니오      |
| 기수 정렬   | O(d(n + k))      | ✅ 안정      | ❌ 아니오      |
| 버킷 정렬   | O(n + k) ~ O(n²) | ✅ 구현 가능 | ❌ 일반적 아님 |

#### ✅ 비교 기반 vs 비비교 기반 요약

| 구분             | 비교 기반 정렬    | 비비교 기반 정렬                |
| ---------------- | ----------------- | ------------------------------- |
| 기준             | 값의 비교         | 구조(자릿수, 범위, 고정길이 등) |
| 시간 복잡도 하한 | **O(n log n)**    | **O(n)** 가능                   |
| 데이터 적용 범위 | 범용              | 정수/고정 문자열 등 제한됨      |
| 안정성 여부      | 알고리즘마다 다름 | 대부분 구현 시 안정성 있음      |
| 예시 알고리즘    | 퀵, 병합, 힙 등   | 카운팅, 기수, 버킷 등           |

### ✅ 마무리 요약 표

| 알고리즘    | 비교 기반 | 안정 정렬 | 제자리 정렬 | 시간 복잡도 평균 |
| ----------- | --------- | --------- | ----------- | ---------------- |
| 버블 정렬   | ✅         | ✅         | ✅           | O(n²)            |
| 선택 정렬   | ✅         | ❌         | ✅           | O(n²)            |
| 삽입 정렬   | ✅         | ✅         | ✅           | O(n²)            |
| 퀵 정렬     | ✅         | ❌         | ✅           | O(n log n)       |
| 병합 정렬   | ✅         | ✅         | ❌           | O(n log n)       |
| 힙 정렬     | ✅         | ❌         | ✅           | O(n log n)       |
| 카운팅 정렬 | ❌         | ✅         | ❌           | O(n + k)         |
| 기수 정렬   | ❌         | ✅         | ❌           | O(d(n + k))      |

# 2.3 탐색 알고리즘(Searching)

## 선형 탐색(Linear Search)

### 1. 개요

**선형 탐색(Linear Search)**은 배열의 첫 번째 요소부터 시작하여
 **찾는 값과 일치하는지 하나씩 비교**하면서 탐색하는 알고리즘이다.

> 📌 가장 단순한 검색 방법
>
> - 정렬 여부에 **무관하게 사용 가능**
> - **최악의 경우 전체 탐색** 필요

### 2. 동작 방식

#### 예: 배열 `[7, 2, 9, 4, 6]`에서 4를 찾는 경우

- 1단계: 7 → X
- 2단계: 2 → X
- 3단계: 9 → X
- 4단계: 4 → ✅ 찾음!

### 3. C 언어 구현

```
#include <stdio.h>

int linearSearch(int arr[], int n, int target) {
    for (int i = 0; i < n; i++) {
        if (arr[i] == target)
            return i;  // 인덱스 반환
    }
    return -1;  // 찾지 못한 경우
}
```

#### 사용 예시

```
int main() {
    int data[] = {5, 3, 8, 6, 7};
    int n = sizeof(data) / sizeof(data[0]);
    int target = 6;

    int index = linearSearch(data, n, target);

    if (index != -1)
        printf("Found at index: %d\n", index);
    else
        printf("Not found\n");

    return 0;
}
```

### 4. 시간 복잡도

| 경우 | 비교 횟수               | 시간 복잡도 |
| ---- | ----------------------- | ----------- |
| 최선 | 1 (첫 번째에 존재할 때) | **O(1)**    |
| 평균 | n/2                     | **O(n)**    |
| 최악 | n (맨 끝 or 없음)       | **O(n)**    |

### 5. 공간 복잡도

- **O(1)**
   → 단일 루프와 정수 변수만 사용 (추가 메모리 없음)

### 6. 장단점

| ✅ 장점                         | ❌ 단점                       |
| ------------------------------ | ---------------------------- |
| 구현이 매우 간단함             | 시간복잡도가 비효율적 $O(n)$ |
| 정렬 여부와 무관하게 사용 가능 | 대량 데이터에 비효율적       |
| 작은 데이터셋에서 유용         | 이진 탐색에 비해 성능 낮음   |

### 7. 사용 시기

- 배열이 **작고**, 정렬되지 않았을 때
- 빠르게 구현해야 할 때
- 정렬 자체가 불가능한 데이터 구조일 때 (ex. 연결 리스트)

### 8. 비교: 선형 탐색 vs 이진 탐색

| 항목        | 선형 탐색                 | 이진 탐색                   |
| ----------- | ------------------------- | --------------------------- |
| 필요 조건   | 정렬 불필요               | 배열이 정렬되어 있어야 함   |
| 시간 복잡도 | O(n)                      | O(log n)                    |
| 구조        | 반복/순차적 비교          | 중간값 기준으로 절반씩 제거 |
| 대상 구조   | 배열, 리스트, 일반 구조체 | 정렬된 배열만 가능          |
| 사용 난이도 | 매우 쉬움                 | 비교적 구조적 이해 필요     |

### ✅ 결론

- 선형 탐색은 **가장 단순하고 직관적인 탐색 알고리즘**이다.
- 성능은 낮지만 **유연성, 범용성**은 뛰어나며,
   **정렬이 불가능한 상황이나 간단한 데이터 탐색**에는 매우 적합하다.
- 그러나 **입력 크기가 커지면** 이진 탐색이나 해시 기반 탐색이 더 유리하다.

## 이진 탐색(Binary Search)

### 1. 개요

**이진 탐색(Binary Search)**은 **정렬된 배열**에서 **중간값을 기준으로 좌우 절반씩 탐색 범위를 줄여가며** 원하는 값을 찾는 알고리즘이다.
 탐색 범위를 **한 번 비교할 때마다 절반으로 줄일 수 있기 때문에**, 매우 빠르게 탐색 가능하다.

> ⚠️ 전제 조건:
>  **탐색 대상 배열이 오름차순 등으로 정렬되어 있어야 함**

### 2. 동작 방식 예시

배열: `[2, 4, 6, 8, 10, 12, 14]`
 찾을 값: `10`

1. mid = 3 → arr[3] = 8 → 10 > 8 → 오른쪽 반 탐색
2. mid = 5 → arr[5] = 12 → 10 < 12 → 왼쪽 반 탐색
3. mid = 4 → arr[4] = 10 → ✅ 찾음!

### 3. C 언어 구현

#### 3.1 반복문 기반

```
int binarySearch(int arr[], int n, int target) {
    int left = 0;
    int right = n - 1;

    while (left <= right) {
        int mid = left + (right - left) / 2;

        if (arr[mid] == target)
            return mid;
        else if (arr[mid] < target)
            left = mid + 1;
        else
            right = mid - 1;
    }

    return -1;  // 찾지 못한 경우
}
```

#### 3.2 재귀 기반

```
int binarySearchRecursive(int arr[], int left, int right, int target) {
    if (left > right)
        return -1;

    int mid = left + (right - left) / 2;

    if (arr[mid] == target)
        return mid;
    else if (arr[mid] < target)
        return binarySearchRecursive(arr, mid + 1, right, target);
    else
        return binarySearchRecursive(arr, left, mid - 1, target);
}
```

### 4. 시간 복잡도

| 경우               | 비교 횟수 (최대) | 시간 복잡도  |
| ------------------ | ---------------- | ------------ |
| 최선 (중간에 발견) | 1                | **O(1)**     |
| 평균/최악          | $\log_2 n$       | **O(log n)** |

> 매 단계마다 탐색 공간이 절반으로 줄어드므로 **로그 기반 시간 복잡도**를 가진다.

### 5. 공간 복잡도

- 반복형: **O(1)**
- 재귀형: **O(log n)** (재귀 호출 스택 사용)

### 6. 장단점 정리

| ✅ 장점                    | ❌ 단점                                |
| ------------------------- | ------------------------------------- |
| 매우 빠른 탐색 (O(log n)) | **정렬된 배열**이 필요함              |
| 구현이 단순하며 효율적    | 삽입/삭제가 많은 동적 배열에는 부적합 |
| 반복/재귀 둘 다 가능      | 링크드 리스트 구조에는 사용 불가      |

### 7. 선형 탐색과 이진 탐색 비교

| 항목           | 선형 탐색       | 이진 탐색                      |
| -------------- | --------------- | ------------------------------ |
| 정렬 필요 여부 | ❌ 불필요        | ✅ 반드시 필요                  |
| 시간 복잡도    | O(n)            | O(log n)                       |
| 구현 난이도    | 매우 쉬움       | 중간 정도 (정렬, 인덱스 주의)  |
| 자료구조       | 배열, 리스트 등 | **배열 기반 정렬 구조**만 가능 |

### 8. 응용 분야

- 사전(Dictionary) 검색
- 정렬된 로그 데이터 탐색
- 데이터베이스 인덱스 탐색
- 알고리즘 이진 결정 (Lower Bound / Upper Bound)

### 9. Lower Bound & Upper Bound

> 정렬된 배열에서 어떤 값 이상/초과 위치를 찾을 때 활용됨

```
// Lower Bound: 값 이상 첫 번째 위치
int lowerBound(int arr[], int n, int target) {
    int left = 0, right = n;
    while (left < right) {
        int mid = left + (right - left) / 2;
        if (arr[mid] < target)
            left = mid + 1;
        else
            right = mid;
    }
    return left;
}
```

### ✅ 결론

- **이진 탐색은 정렬된 배열에서 가장 효율적인 탐색 알고리즘 중 하나**
- 입력이 정렬되어 있다는 조건이 충족되면 **O(log n)** 시간으로 매우 빠르게 동작
- **삽입/삭제 빈도가 낮고 조회가 잦은 상황**에서 이상적

### 다음 주제

- **Lower/Upper Bound의 실제 예제**
- **이진 탐색의 활용 예 (파라메트릭 서치)**
- **문자열 정렬 후 이진 탐색**
- **정렬 전처리 포함 이진 탐색 전체 구현**

## 이진 탐색 트리 탐색

### 1. 📌 이진 탐색 트리(BST)란?

**이진 탐색 트리 (Binary Search Tree)**는 다음과 같은 **조건을 만족하는 이진 트리 구조**이다:

- **왼쪽 서브트리**의 값 < 현재 노드 값
- **오른쪽 서브트리**의 값 > 현재 노드 값
- **모든 서브트리도 이진 탐색 트리여야 함**

> 이러한 규칙을 통해 빠른 탐색, 삽입, 삭제가 가능해짐

### 2. 🔧 C 언어 구조체 정의

```
#include <stdio.h>
#include <stdlib.h>

typedef struct TreeNode {
    int data;
    struct TreeNode* left;
    struct TreeNode* right;
} TreeNode;
```

### 3. 🔍 탐색 함수 (반복형 & 재귀형)

#### 📍 3.1 반복적 탐색 (Iterative Search)

```
TreeNode* searchIterative(TreeNode* root, int key) {
    while (root != NULL) {
        if (key == root->data)
            return root;
        else if (key < root->data)
            root = root->left;
        else
            root = root->right;
    }
    return NULL;
}
```

#### 📍 3.2 재귀적 탐색 (Recursive Search)

```
TreeNode* searchRecursive(TreeNode* root, int key) {
    if (root == NULL || root->data == key)
        return root;

    if (key < root->data)
        return searchRecursive(root->left, key);
    else
        return searchRecursive(root->right, key);
}
```

### 4. ✅ 삽입 함수 (테스트용)

```
TreeNode* insert(TreeNode* root, int key) {
    if (root == NULL) {
        TreeNode* node = (TreeNode*)malloc(sizeof(TreeNode));
        node->data = key;
        node->left = node->right = NULL;
        return node;
    }

    if (key < root->data)
        root->left = insert(root->left, key);
    else if (key > root->data)
        root->right = insert(root->right, key);

    return root;
}
```

### 5. 🧠 탐색 예시

```
int main() {
    TreeNode* root = NULL;
    root = insert(root, 8);
    root = insert(root, 3);
    root = insert(root, 10);
    root = insert(root, 1);
    root = insert(root, 6);
    root = insert(root, 14);

    int key = 6;
    TreeNode* found = searchRecursive(root, key);

    if (found != NULL)
        printf("Found node with value: %d\n", found->data);
    else
        printf("Value not found.\n");

    return 0;
}
```

### 6. 📈 시간 복잡도

| 트리 상태          | 시간 복잡도  |
| ------------------ | ------------ |
| **균형 잡힌 트리** | **O(log n)** |
| **편향된 트리**    | **O(n)**     |

> 🔻 예: 모든 노드가 오른쪽으로만 연결된 경우 → 선형 탐색과 동일해짐

### 7. 🧩 공간 복잡도

- 반복형: **O(1)**
- 재귀형: **O(h)** (트리 높이만큼의 콜스택)

### 8. ⚠️ BST 탐색 주의사항

- BST는 **중복을 허용하지 않는 경우가 일반적**
- 입력 데이터가 정렬되어 있으면 **편향 트리 발생 → 성능 저하**
   → AVL 트리, Red-Black 트리 등으로 **균형 보정 필요**

### ✅ 결론

- 이진 탐색 트리는 정렬된 배열보다 **더 유연한 탐색 구조**를 제공
- 탐색 성능은 **트리의 균형도에 의존**
- 반복형과 재귀형 모두 사용 가능하며, 재귀형이 더 직관적

### 다음 주제

- **최솟값/최댓값 탐색**,
- **BST 삭제 알고리즘**,
- **균형 이진 탐색 트리 (AVL Tree)**,
- **이진 탐색 트리 순회 (Inorder, Preorder, Postorder)**

## DFS(Depth-First Search), BFS(Breadth-First Search)

### 🟦 1. DFS (깊이 우선 탐색)

#### 📌 개념

DFS는 **가장 깊은 노드까지 먼저 탐색**한 다음, 더 이상 진행할 수 없으면 **되돌아와서 다른 경로로 진행**하는 방식이다.

> ✔ 구현 방식: **스택** 사용 (보통은 **재귀**)

#### 🔧 C 언어 구현 (인접 리스트 기반)

```
#define MAX_V 100

typedef struct Node {
    int vertex;
    struct Node* next;
} Node;

Node* adj[MAX_V];
int visited[MAX_V];

void dfs(int v) {
    visited[v] = 1;
    printf("%d ", v);
    for (Node* p = adj[v]; p != NULL; p = p->next) {
        if (!visited[p->vertex])
            dfs(p->vertex);
    }
}
```

#### ✅ DFS 특징

| 항목          | 내용                                          |
| ------------- | --------------------------------------------- |
| 사용 자료구조 | 스택 or 재귀 콜 스택                          |
| 탐색 순서     | 한 방향으로 **끝까지** 내려감                 |
| 최단 경로     | ❌ 보장되지 않음                               |
| 주요 사용 예  | 경로 찾기, 미로 탐색, 백트래킹, 순열, 조합 등 |
| 공간 효율     | 높음 (보통 큐를 안 쓰므로)                    |

### 🟨 2. BFS (너비 우선 탐색)

#### 📌 개념

BFS는 **시작 노드에 가까운 노드부터 탐색**하며, **동일 거리 레벨을 모두 처리한 후 다음 레벨로 넘어가는** 방식이다.

> ✔ 구현 방식: **큐(Queue)** 사용

#### 🔧 C 언어 구현 (인접 리스트 기반)

```
void bfs(int start) {
    int queue[MAX_V];
    int front = 0, rear = 0;

    for (int i = 0; i < MAX_V; i++) visited[i] = 0;

    visited[start] = 1;
    queue[rear++] = start;

    while (front < rear) {
        int v = queue[front++];
        printf("%d ", v);
        for (Node* p = adj[v]; p != NULL; p = p->next) {
            if (!visited[p->vertex]) {
                visited[p->vertex] = 1;
                queue[rear++] = p->vertex;
            }
        }
    }
}
```

#### ✅ BFS 특징

| 항목          | 내용                                             |
| ------------- | ------------------------------------------------ |
| 사용 자료구조 | 큐(Queue)                                        |
| 탐색 순서     | 가까운 노드부터 → 레벨별로 확장                  |
| 최단 경로     | ✅ **보장됨** (가중치 없는 그래프에 한함)         |
| 주요 사용 예  | 최단거리 탐색, 네트워크 흐름, 레벨 구분, 전파 등 |
| 공간 효율     | 낮음 (큐에 많은 노드를 동시에 저장할 수 있음)    |

### 📊 DFS vs BFS 비교 요약

| 항목             | DFS (깊이 우선 탐색)            | BFS (너비 우선 탐색)              |
| ---------------- | ------------------------------- | --------------------------------- |
| 자료구조 사용    | 스택 (재귀 호출 포함)           | 큐                                |
| 탐색 순서        | 깊이 우선 (끝까지 간 후 백트랙) | 레벨 순서 (가까운 것부터)         |
| 최단 경로 보장   | ❌ 없음                          | ✅ 있음                            |
| 메모리 사용      | 보통 낮음                       | 큐가 클 수 있어 상대적으로 큼     |
| 재귀로 구현 가능 | ✅ 자연스럽게 가능               | ❌ (보통 반복문만 사용)            |
| 적합한 문제 유형 | 백트래킹, 경로 탐색, 순열/조합  | 최단 거리, 레벨 처리, 연결성 탐색 |

### 📈 시간 및 공간 복잡도

| 항목        | DFS                       | BFS                     |
| ----------- | ------------------------- | ----------------------- |
| 시간 복잡도 | **O(V + E)**              | **O(V + E)**            |
| 공간 복잡도 | **O(V)** (visited + 재귀) | **O(V)** (visited + 큐) |

- V: 정점 개수
- E: 간선 개수

### 🧠 예시 그래프 순회 결과

그래프:

```
   0
  / \
 1   2
 |   |
 3   4
```

- DFS(0) → `0 1 3 2 4`
- BFS(0) → `0 1 2 3 4`

### ✅ 결론

| 상황                                | 추천 알고리즘 |
| ----------------------------------- | ------------- |
| **경로/백트래킹/모든 경로 순회**    | DFS           |
| **최단 거리/가까운 거리 우선 탐색** | BFS           |
| **조합/순열, 퍼즐, 미로 탐색**      | DFS           |
| **그래프에서 최소 단계 도달 여부**  | BFS           |

### 다음 주제

- DFS/BFS로 **사이클 탐지**,
- DFS로 **조합/순열 생성**,
- BFS로 **최단거리 경로 재구성**,
- DFS/BFS **트리 순회 비교 (Inorder 등)**

# 2.4 분할 정복(Divide and Conquer)

## 개념 및 적용 패턴

> 문제를 **작은 부분 문제로 나누고**, 각각을 **재귀적으로 해결**한 후, 결과를 **결합(Combine)**하여 전체 문제를 푸는 알고리즘 기법

### 🟦 1. 핵심 개념

#### 📌 정의

분할 정복은 문제를 **동일한 형태의 더 작은 문제들로 분할**하고,
 각 문제를 재귀적으로 해결한 뒤, **결과를 합쳐서 전체 문제를 해결**하는 방식이다.

#### 📌 일반 구조

```
function solve(problem):
    if (problem is small enough):      ← 기저 조건 (Base Case)
        return direct_solution(problem)

    subproblems = divide(problem)      ← 분할 (Divide)
    results = []

    for each sub in subproblems:       ← 정복 (Conquer)
        results.append(solve(sub))

    return combine(results)            ← 결합 (Combine)
```

### 🧩 2. 적용 패턴

#### ✅ 대표적 적용 패턴 요약

| 패턴/유형              | 설명                                                | 예시 알고리즘                                              |
| ---------------------- | --------------------------------------------------- | ---------------------------------------------------------- |
| **정렬**               | 큰 배열 → 작은 배열 → 병합                          | 병합 정렬 (Merge Sort), 퀵 정렬 (Quick Sort)               |
| **탐색**               | 중간값 기준 좌우로 나눠 탐색                        | 이진 탐색 (Binary Search)                                  |
| **배열 분할/정복**     | 최대, 최소, 합 등 배열을 반으로 나눠서 처리         | 분할된 배열 최대합 구하기, 선형 최대 구간 합 (Kadane 변형) |
| **행렬 분할**          | 행렬을 4개 조각으로 나누고 각각 처리                | Strassen 행렬 곱 알고리즘                                  |
| **지리적 분할**        | 평면을 사분면/팔분면 등으로 나눠 처리               | 최근접 쌍 찾기 (Closest Pair of Points)                    |
| **정수 문제 분할**     | 큰 정수를 절반으로 나눠 처리                        | 거듭제곱 빠르게 계산, Karatsuba 곱셈                       |
| **재귀적 그래프 분할** | 그래프를 절반으로 나누고 각각 분석                  | 퇴각 탐색 기반 문제, 연결 요소 탐색 등                     |
| **다항식/FFT 분할**    | 다항식을 절반으로 나눠서 재귀적으로 곱셈, 변환 수행 | FFT (고속 푸리에 변환)                                     |

### 📈 3. 시간 복잡도 분석 방식 (마스터 정리)

분할 정복 알고리즘은 **재귀 점화식**으로 시간 복잡도를 분석한다:

```
T(n) = aT(n/b) + O(n^d)
```

- a: 분할된 문제 수
- b: 문제 크기 축소 비율
- d: 분할/결합의 시간 소모 정도

#### 📌 마스터 정리 (Master Theorem)

| 조건      | 시간 복잡도       |
| --------- | ----------------- |
| $a > b^d$ | $O(n^{\log_b a})$ |
| $a = b^d$ | $O(n^d \log n)$   |
| $a < b^d$ | $O(n^d)$          |

> 예: 병합 정렬
>
> - 점화식: $T(n) = 2T(n/2) + O(n)$
> - 적용: $a = 2, b = 2, d = 1$ → $a = b^d$
> - 결과: $O(n \log n)$

### 🧠 4. 분할 정복 vs 동적 프로그래밍

| 항목           | 분할 정복                        | 동적 프로그래밍                      |
| -------------- | -------------------------------- | ------------------------------------ |
| 중복 하위 문제 | ❌ 거의 없음                      | ✅ 많음                               |
| 하위 결과 저장 | ❌ 저장하지 않음                  | ✅ 저장하여 재활용                    |
| 최적 부분 구조 | ✅ 필요                           | ✅ 필요                               |
| 대표 알고리즘  | 병합 정렬, 퀵 정렬, 이진 탐색 등 | 피보나치 수, 배낭 문제, 최단 경로 등 |

> 💡 **중복되는 하위 문제**가 있다면 → **동적 프로그래밍** 고려!

### 📦 5. 대표 알고리즘 예시 정리

| 알고리즘          | 설명                                     | 시간 복잡도                 |
| ----------------- | ---------------------------------------- | --------------------------- |
| **Merge Sort**    | 배열을 반으로 나눠 정렬 후 병합          | O(n log n)                  |
| **Quick Sort**    | 피벗 기준으로 분할하여 재귀적으로 정렬   | O(n log n) 평균, O(n²) 최악 |
| **Binary Search** | 중간값과 비교하여 절반으로 나눠 탐색     | O(log n)                    |
| **Closest Pair**  | 좌표 정렬 후 좌/우로 나눠 최근접 쌍 찾기 | O(n log n)                  |
| **Strassen**      | 행렬 곱을 분할 후 최소 곱셈 수로 계산    | O(n^2.81)                   |
| **Karatsuba**     | 정수 곱셈을 분할하여 빠르게 계산         | O(n^1.585)                  |
| **FFT**           | 다항식 분할 후 푸리에 변환 수행          | O(n log n)                  |

### ✅ 결론 요약

| 핵심 요소 | 설명                                                         |
| --------- | ------------------------------------------------------------ |
| Divide    | 문제를 절반 혹은 더 작은 단위로 나눔                         |
| Conquer   | 각 문제를 재귀적으로 해결                                    |
| Combine   | 부분 결과들을 하나로 합쳐 전체 문제 해결                     |
| 사용 조건 | 문제를 나누는 방식이 자연스럽고, 하위 문제가 독립적일 때 가장 효과적 |
| 주요 장점 | 시간복잡도 감소, 병렬 처리 가능성, 자연스러운 재귀 구조 적용 가능 |

### 다음 주제

- **실전 예제 코드 구현 (Merge Sort, Closest Pair 등)**
- **분할 정복 + DP 혼합 전략**
- **트리 기반 문제에서의 분할 정복 응용**

## 예시: Merge Sort, Quick Sort, Karatsuba Multiplication

### ✅ 1. Merge Sort (병합 정렬)

#### 📌 아이디어

- 배열을 **반으로 계속 나눔**
- 각각을 정렬한 후, **두 배열을 병합**
- 안정 정렬이고, **최악에도 O(n log n)**

#### 🔧 C 코드 예시

```
void merge(int arr[], int left, int mid, int right) {
    int i, j, k;
    int n1 = mid - left + 1;
    int n2 = right - mid;

    int L[n1], R[n2];

    for (i = 0; i < n1; i++) L[i] = arr[left + i];
    for (j = 0; j < n2; j++) R[j] = arr[mid + 1 + j];

    i = j = 0;
    k = left;

    while (i < n1 && j < n2) {
        arr[k++] = (L[i] <= R[j]) ? L[i++] : R[j++];
    }

    while (i < n1) arr[k++] = L[i++];
    while (j < n2) arr[k++] = R[j++];
}

void mergeSort(int arr[], int left, int right) {
    if (left < right) {
        int mid = (left + right) / 2;

        mergeSort(arr, left, mid);       // 왼쪽 정렬
        mergeSort(arr, mid + 1, right);  // 오른쪽 정렬
        merge(arr, left, mid, right);    // 병합
    }
}
```

#### 🧠 시간 복잡도

- 분할 단계: O(log n)
- 병합 단계: O(n)
- **전체 시간복잡도: O(n log n)**
- 공간복잡도: O(n) (보조 배열 사용)

### ✅ 2. Quick Sort (퀵 정렬)

#### 📌 아이디어

- 배열에서 **피벗(pivot)**을 선택
- 피벗보다 작은 요소는 왼쪽, 큰 요소는 오른쪽으로 분할
- 재귀적으로 각 부분 정렬

#### 🔧 C 코드 예시

```
int partition(int arr[], int low, int high) {
    int pivot = arr[high];
    int i = low - 1;

    for (int j = low; j < high; j++) {
        if (arr[j] < pivot) {
            i++;
            int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;
        }
    }
    int tmp = arr[i+1]; arr[i+1] = arr[high]; arr[high] = tmp;
    return i + 1;
}

void quickSort(int arr[], int low, int high) {
    if (low < high) {
        int pi = partition(arr, low, high);  // 피벗 위치
        quickSort(arr, low, pi - 1);         // 왼쪽 정렬
        quickSort(arr, pi + 1, high);        // 오른쪽 정렬
    }
}
```

#### 🧠 시간 복잡도

| 경우             | 복잡도     |
| ---------------- | ---------- |
| 평균/최선        | O(n log n) |
| 최악 (편향 분할) | O(n²)      |

> 공간복잡도: O(log n) (스택 호출 깊이)

### ✅ 3. Karatsuba Multiplication (카라츠바 정수 곱셈)

#### 📌 아이디어

두 n자리 정수를 곱할 때, 단순 곱셈은 **O(n²)**
 → 이를 3번의 곱셈으로 줄여 **O(n^1.585)**로 개선

- $x = a \cdot 10^m + b$
- $y = c \cdot 10^m + d$
- $xy = ac \cdot 10^{2m} + ((a+b)(c+d) - ac - bd) \cdot 10^m + bd$

#### 🔧 C 코드 (정수 단위, 문자열 처리 생략)

```
int karatsuba(int x, int y) {
    if (x < 10 || y < 10)
        return x * y;

    int n = fmax(log10(x), log10(y)) + 1;
    int m = n / 2;
    int pow10 = pow(10, m);

    int a = x / pow10;
    int b = x % pow10;
    int c = y / pow10;
    int d = y % pow10;

    int ac = karatsuba(a, c);
    int bd = karatsuba(b, d);
    int abcd = karatsuba(a + b, c + d);

    return ac * pow10 * pow10 + (abcd - ac - bd) * pow10 + bd;
}
```

> 위는 간단화를 위한 정수 기반 예시야. 실제 구현에서는 **문자열 정수 처리**로 n자리 이상 숫자를 다뤄야 해.

#### 🧠 시간 복잡도

- 점화식: $T(n) = 3T(n/2) + O(n)$
- 결과: **$$O(n^log₂3) ≈ O(n^1.585)$$**

### ✅ 요약 비교

| 알고리즘   | 핵심 동작                     | 시간 복잡도                 | 공간 복잡도 | 특징                                 |
| ---------- | ----------------------------- | --------------------------- | ----------- | ------------------------------------ |
| Merge Sort | 분할 → 정렬 → 병합            | O(n log n)                  | O(n)        | 안정 정렬, 최악에도 빠름             |
| Quick Sort | 분할(피벗) → 정렬             | 평균 O(n log n), 최악 O(n²) | O(log n)    | 실전 최강 성능, 불안정 정렬          |
| Karatsuba  | 자릿수 분할 → 곱셈 3회 → 병합 | O(n^1.585)                  | O(n)        | 큰 정수 곱셈에 특화된 분할 정복 기법 |

# 2.5 탐욕 알고리즘(Greedy Algorithm)

## 선택 기준 정의

### 🔍 핵심 고려 요소 6가지

| 요소               | 설명                                                         |
| ------------------ | ------------------------------------------------------------ |
| ✅ 입력 크기        | 데이터가 작거나 클 때 선호하는 알고리즘이 다름               |
| ✅ 데이터 정렬도    | 거의 정렬되어 있는지, 무작위인지, 역정렬인지                 |
| ✅ 안정성 필요 여부 | 동일한 값의 원소 순서가 보존되어야 하는지                    |
| ✅ 추가 메모리 허용 | 보조 배열 사용 가능한지 (제자리 정렬 필요 여부)              |
| ✅ 최악 시간 보장   | 항상 일정 시간복잡도를 요구하는지 (예: 실시간, 예측성 있는 처리) |
| ✅ 자료 구조 특성   | 연결 리스트인지, 배열인지, 파일 기반 외부 데이터인지 등      |

### 🧠 알고리즘별 선택 가이드

#### ✅ 1. Merge Sort (병합 정렬)

| 기준                    | 선택 이유                                 |
| ----------------------- | ----------------------------------------- |
| 안정 정렬 필요          | ✅ 동일 값의 상대 순서 유지 가능           |
| 최악 성능 보장 필요     | ✅ 항상 O(n log n) 성능 유지               |
| 외부 정렬 (디스크 기반) | ✅ 연속적인 merge 처리로 큰 파일 처리 가능 |
| 메모리 사용 가능        | ❌ O(n) 보조 배열 필요                     |
| 링크드 리스트 정렬      | ✅ 병합 정렬이 연결 리스트에도 효과적      |

→ **데이터가 크고 안정성이 필요하면 병합 정렬**

#### ✅ 2. Quick Sort (퀵 정렬)

| 기준                | 선택 이유                                     |
| ------------------- | --------------------------------------------- |
| 평균 성능 최상      | ✅ 매우 빠름 (실전에서 가장 빠른 정렬 중 하나) |
| 메모리 제한 환경    | ✅ 제자리 정렬 (in-place) 가능                 |
| 안정성 필요 없음    | ❌ 불안정 정렬                                 |
| 최악 성능 회피 필요 | ❌ 최악 O(n²) (→ 피벗 개선 필요)               |

→ **빠른 정렬이 필요한 경우, 정렬 안정성이 불필요하면 퀵 정렬**

#### ✅ 3. Heap Sort (힙 정렬)

| 기준                 | 선택 이유                   |
| -------------------- | --------------------------- |
| 제자리 정렬 필요     | ✅ O(1) 보조 메모리          |
| 최악 시간 보장 필요  | ✅ 항상 O(n log n)           |
| 안정성 불필요        | ❌ 불안정 정렬               |
| 성능보다 예측성 우선 | ✅ 실시간 시스템 등에서 유리 |

→ **메모리 사용을 최소화하고, 일정한 시간 내에 처리해야 할 때 힙 정렬**

#### ✅ 4. Insertion Sort (삽입 정렬)

| 기준               | 선택 이유                                      |
| ------------------ | ---------------------------------------------- |
| 입력 크기가 작음   | ✅ n < 20~30에서는 Merge/Quick보다 빠를 수 있음 |
| 거의 정렬된 상태임 | ✅ 시간복잡도 O(n)에 가까움                     |
| 안정성이 필요함    | ✅ 가능                                         |
| 공간 사용 제한     | ✅ O(1)                                         |

→ **작고 거의 정렬된 데이터에 가장 적합**

#### ✅ 5. Counting / Radix Sort (비교 기반 아님)

| 기준                       | 선택 이유                                      |
| -------------------------- | ---------------------------------------------- |
| 정수 데이터 정렬           | ✅ 정수, 고정 길이 문자열 등에 매우 빠름 (O(n)) |
| 데이터 범위 작음           | ✅ 카운팅 정렬은 값 범위가 작을 때 최적         |
| 안정성이 필요함            | ✅ 카운팅/기수 정렬 모두 안정 정렬              |
| 범위가 너무 큼 / 음수 있음 | ❌ 메모리 낭비 발생 가능, 확장 구현 필요        |

→ **정수 기반 데이터에서 초고속 정렬이 필요할 때 선택**

#### ✅ 6. Karatsuba Multiplication

| 기준                       | 선택 이유                              |
| -------------------------- | -------------------------------------- |
| 큰 정수의 곱셈 필요        | ✅ n자리 이상의 큰 수 계산에 효율적     |
| 속도가 매우 중요함         | ✅ O(n^1.585)로 단순 곱셈보다 훨씬 빠름 |
| 문자열 기반 숫자 처리 필요 | ✅ 자릿수 분할 알고리즘과 연결 가능     |

→ **BigInteger 곱셈에서 일반 곱셈보다 훨씬 빠른 알고리즘**

### 🧩 선택 전략 요약 표

| 상황/조건                           | 추천 알고리즘            |
| ----------------------------------- | ------------------------ |
| 입력이 거의 정렬됨                  | Insertion Sort           |
| 정렬 안정성 필요                    | Merge / Counting / Radix |
| 입력 크기가 작음                    | Insertion / Selection    |
| 평균 성능이 가장 중요함             | Quick Sort               |
| 메모리 사용이 제한됨                | Heap Sort / Quick Sort   |
| 실시간 시스템 (최악 시간 보장 필요) | Merge / Heap Sort        |
| 숫자 범위가 작고 정수형 데이터      | Counting / Radix Sort    |
| 외부 정렬 (파일 정렬 등)            | Merge Sort               |
| 매우 큰 정수 곱셈                   | Karatsuba                |

### ✅ 마무리 요약

| 기준             | 중요 알고리즘                                |
| ---------------- | -------------------------------------------- |
| 안정성           | 병합 정렬, 삽입 정렬, 카운팅 정렬, 기수 정렬 |
| 메모리 효율      | 힙 정렬, 퀵 정렬 (제자리)                    |
| 최악 시간 보장   | 병합 정렬, 힙 정렬                           |
| 빠른 평균 성능   | 퀵 정렬 (피벗 전략 포함 시)                  |
| 정수/문자열 전용 | 기수 정렬, 카운팅 정렬, Karatsuba            |
| 재귀적 분할 정복 | 병합 정렬, 퀵 정렬, Karatsuba, FFT 등        |

### 다음 주제

- **Timsort(삽입 + 병합 정렬) 하이브리드 구조 분석**
- **Quick Sort의 피벗 전략 비교**
- **실전 상황별 알고리즘 선택 시뮬레이션**

## 대표 예시: Kruskal, Prim, Huffman Encoding

### ✅ 1. Kruskal 알고리즘

> **모든 간선을 가중치 순으로 정렬 후**, 사이클을 만들지 않도록 하나씩 선택하여 MST(최소 신장 트리)를 구성하는 알고리즘

#### 📌 핵심 개념

- **간선 중심 접근**
- 간선을 **가중치 오름차순 정렬**
- **사이클이 생기지 않도록 선택** (→ Union-Find 사용)
- 총 V-1개의 간선을 선택하면 완료

#### 🔧 C 스타일 주요 흐름

```
struct Edge {
    int u, v, weight;
};

int find(int parent[], int x) {
    if (parent[x] != x)
        parent[x] = find(parent, parent[x]);
    return parent[x];
}

void unionSet(int parent[], int x, int y) {
    int rx = find(parent, x);
    int ry = find(parent, y);
    if (rx != ry)
        parent[ry] = rx;
}
```

> 정렬 + Union-Find로 구현

#### 📈 시간 복잡도

- 간선 정렬: $O(E \log E)$
- Union-Find (Path Compression): $O(\alpha(V))$
- 전체: **$O(E \log E)$**

#### ✅ 특징

| 항목          | 설명                         |
| ------------- | ---------------------------- |
| 탐색 중심     | 간선 중심 (Edge-based)       |
| 구현 난이도   | 정렬 + 유니온 파인드 필요    |
| 유리한 경우   | 희소 그래프 (간선이 적을 때) |
| 알고리즘 유형 | 그리디 + Union-Find          |

### ✅ 2. Prim 알고리즘

> 하나의 시작 정점에서부터 **가장 가까운 정점**을 하나씩 연결하면서 MST를 구성하는 알고리즘

#### 📌 핵심 개념

- **정점 중심 접근**
- 시작 정점에서부터 **가장 짧은 간선으로 확장**
- 최소 힙 (우선순위 큐)을 사용하여 매 단계 최적 선택

#### 🔧 핵심 구조 (우선순위 큐 방식)

```
priority_queue<pair<int, int>, vector<pair<int, int>>, greater<>> pq;
int dist[V]; // 최단 간선 비용
bool visited[V]; // 방문 여부
```

- 매 단계마다 `min(dist)`인 정점을 선택하여 MST에 추가

#### 📈 시간 복잡도

| 구현 방식        | 복잡도            |
| ---------------- | ----------------- |
| 배열 방식        | $O(V^2)$          |
| 힙 + 인접 리스트 | **$O(E \log V)$** |

#### ✅ 특징

| 항목          | 설명                             |
| ------------- | -------------------------------- |
| 탐색 중심     | 정점 중심 (Vertex-based)         |
| 구현 난이도   | 우선순위 큐 활용                 |
| 유리한 경우   | **조밀 그래프** (간선이 많을 때) |
| 알고리즘 유형 | 그리디 + 다익스트라와 유사 구조  |

### ✅ 3. Huffman 인코딩

> 가장 빈도 높은 문자는 **짧은 비트**, 드문 문자는 **긴 비트**로 인코딩하여 전체 메시지를 **최소 길이**로 만드는 알고리즘

#### 📌 핵심 개념

- **문자의 빈도수**를 기반으로 트리 구성
- **가장 빈도 낮은 두 문자를 하나의 부모 노드로 합침**
- 그리디 방식으로 트리를 아래에서 위로 구성
- 각 문자의 인코딩 길이를 줄이면서 **최소 기대 비트 수** 달성

#### 🔧 구조 및 자료구조

- **우선순위 큐(min-heap)** 사용
- `Node { 문자, 빈도, 왼쪽 자식, 오른쪽 자식 }`
- **리프 노드 → 문자 / 내부 노드 → 결합된 빈도**

```
while (큐 크기 > 1):
    노드1 = 최소 꺼냄
    노드2 = 두 번째 최소 꺼냄
    새로운 내부 노드 = 노드1 + 노드2
    큐에 삽입
```

#### 📈 시간 복잡도

- 노드 수: n
- 힙 삽입/제거 반복: O(n log n)

#### ✅ 특징

| 항목          | 설명                                        |
| ------------- | ------------------------------------------- |
| 최적 부호화   | 무손실 압축 방식으로 이론적 최적에 가까움   |
| 안정성        | 빈도가 같아도 인코딩 길이 동등함            |
| 사용 예       | ZIP, JPEG, PNG 등 다양한 압축 포맷에 사용됨 |
| 알고리즘 유형 | 그리디 알고리즘 + 힙 구조 활용              |

### 🧠 비교 요약

| 알고리즘 | 문제 유형             | 탐색 대상     | 자료구조          | 시간 복잡도   | 알고리즘 유형         |
| -------- | --------------------- | ------------- | ----------------- | ------------- | --------------------- |
| Kruskal  | MST                   | **간선 중심** | 정렬 + Union-Find | $O(E \log E)$ | Greedy + Disjoint Set |
| Prim     | MST                   | **정점 중심** | 우선순위 큐 (힙)  | $O(E \log V)$ | Greedy                |
| Huffman  | 압축 (최소 비용 트리) | 빈도 기반     | Min-Heap          | $O(n \log n)$ | Greedy + Heap Tree    |

### ✅ 결론

- **Kruskal**: 사이클을 피하며 가장 낮은 간선을 선택 → **희소 그래프**에 적합
- **Prim**: 트리처럼 연결하며 확장 → **조밀 그래프**에 적합
- **Huffman**: 전체 가중 합을 최소로 만드는 최적 부호화 트리

### 다음 주제

- 각 알고리즘의 **구현 코드 (C / Python)**
- **실제 그래프 예제와 시각화된 동작 과정**
- Huffman의 **부호화 / 복호화 과정 전체 흐름**

# 2.6 동적 계획법(Dynamic Programming, DP)

## Overlapping Subproblems, Optimal Substructure

### 🟦 1. Overlapping Subproblems (중복 부분 문제)

#### 📌 정의

문제를 해결하는 과정에서 **동일한 부분 문제가 여러 번 반복해서 등장**하는 성질을 말해.

> 즉, 동일한 계산을 여러 번 하게 되는 상황을 **한 번만 계산해서 저장**하면 효율적임 → **메모이제이션(Memoization)**이나 **테이블 기반 반복(DP Table)**을 사용

#### 🎯 대표 예시: 피보나치 수열

```
fib(5)  
↳ fib(4) + fib(3)  
   ↳ fib(3) + fib(2)  
      ↳ fib(2) + fib(1)  
         ↳ fib(1) + fib(0)
```

- `fib(3)`, `fib(2)`처럼 **같은 함수가 여러 번 호출됨**
- ⇒ **Memoization or DP table**로 중복 계산 제거 가능

#### ✅ 요약 포인트

| 항목             | 설명                                              |
| ---------------- | ------------------------------------------------- |
| 정의             | 동일한 하위 문제를 **반복해서 푸는 경우**         |
| 해결 전략        | **한 번만 계산하고 저장**하여 재활용              |
| 적용 알고리즘 예 | 피보나치 수열, 배낭 문제, 최소 경로, 편집 거리 등 |

### 🟨 2. Optimal Substructure (최적 부분 구조)

#### 📌 정의

문제의 **최적 해답이, 그 부분 문제들의 최적 해답으로 구성될 수 있는 경우**를 의미함.
 즉, **전체 문제의 최적 해결책이 하위 문제들의 최적 해결책을 포함**한다.

#### 🎯 대표 예시: 최단 경로

- A → C의 최단 경로는, 반드시 A → B → C에서
  - A → B의 최단 경로
  - B → C의 최단 경로
     두 개가 **모두 최적이어야** 전체도 최적

#### ✅ 요약 포인트

| 항목             | 설명                                                         |
| ---------------- | ------------------------------------------------------------ |
| 정의             | 전체 문제의 최적 해 = 부분 문제들의 최적 해의 조합           |
| 필요 조건        | **부분 문제의 최적성**이 전체 최적성을 보장해야 함           |
| 적용 알고리즘 예 | 다익스트라, 벨만-포드, 배낭 문제, 최장 증가 부분 수열(LIS), 행렬 곱 최적화 등 |

### ✅ 두 개념의 관계 요약

| 비교 항목                | Overlapping Subproblems               | Optimal Substructure                   |
| ------------------------ | ------------------------------------- | -------------------------------------- |
| 중점                     | **중복 계산 제거**                    | **최적 해를 부분 최적 해로 구성 가능** |
| 저장/재사용 필요성       | 반드시 저장해서 재사용해야 효율적     | 구조적 성질이므로 저장은 필수 아님     |
| 주로 어디서 확인되는가   | 재귀 호출 트리 관찰 (같은 함수 반복)  | 문제 정의나 수학적 관계에서 유도됨     |
| 동적 프로그래밍에서 역할 | 메모이제이션 / 테이블화의 필요성 판단 | 점화식 설계 및 DP 구조 설계의 기반     |

### 🔁 예제 비교

| 문제 유형                   | 중복 부분 문제 있음? | 최적 부분 구조 있음? | DP 적용 가능? |
| --------------------------- | -------------------- | -------------------- | ------------- |
| 피보나치 수열               | ✅                    | ✅                    | ✅             |
| 이진 탐색 (Binary Search)   | ❌                    | ✅                    | ❌             |
| 최단 경로 문제 (다익스트라) | ❌ (중복은 적음)      | ✅                    | 조건부 가능   |
| DP 배낭 문제 (0/1 Knapsack) | ✅                    | ✅                    | ✅             |
| 하노이 탑                   | ❌                    | ✅                    | ❌ (DP 부적합) |

### ✅ 결론

- **Overlapping Subproblems** → 중복을 줄이기 위해 **메모이제이션 or 반복 DP** 적용
- **Optimal Substructure** → DP를 사용하기 위한 **기본 전제 조건**
- 둘 다 만족할 때만 **동적 프로그래밍이 효과적**이다

### 다음 주제

- 실전 문제 예시: **배낭 문제, 편집 거리, 행렬 체인 곱셈, LIS**
- **탑다운/바텀업 비교**
- **그리디와 DP의 충돌 사례 비교**

## Top-down vs Bottom-up 방식

### 🟦 1. Top-down 방식 (메모이제이션 방식)

#### 📌 정의

**재귀 호출**을 기반으로, **문제를 위에서부터 하향식으로 해결**하며
 **이미 계산한 하위 문제는 저장해 재사용하는 방식**이다.

> 즉, 필요한 경우에만 하위 문제를 풀고,
>  이미 푼 문제는 **캐시에 저장**해서 반복 계산을 방지함.

#### 🔧 구조 예시 (피보나치 수열)

```
int dp[100];

int fib(int n) {
    if (n <= 1) return n;
    if (dp[n] != -1) return dp[n];  // 이미 계산된 값

    dp[n] = fib(n - 1) + fib(n - 2);
    return dp[n];
}
```

#### ✅ 특징

| 항목        | 설명                                           |
| ----------- | ---------------------------------------------- |
| 계산 순서   | **큰 문제 → 작은 문제** 순서로 재귀적으로 해결 |
| 메모리 구조 | 보통 **재귀 호출 + 배열 (캐시)** 사용          |
| 계산 범위   | 실제로 필요한 문제만 계산                      |
| 호출 방식   | **재귀 함수 호출** (스택 사용)                 |

### 🟨 2. Bottom-up 방식 (반복문 방식)

#### 📌 정의

**작은 문제부터 차례대로 해결하면서**,
 **그 결과를 테이블에 저장해가며 큰 문제를 완성하는 방식**이다.

> **모든 하위 문제를 미리 계산**한 후, 최종 문제를 해결하는 형태

#### 🔧 구조 예시 (피보나치 수열)

```
int fib(int n) {
    int dp[100];
    dp[0] = 0;
    dp[1] = 1;

    for (int i = 2; i <= n; i++)
        dp[i] = dp[i - 1] + dp[i - 2];

    return dp[n];
}
```

#### ✅ 특징

| 항목        | 설명                                       |
| ----------- | ------------------------------------------ |
| 계산 순서   | **작은 문제 → 큰 문제** 순서로 순차적 계산 |
| 메모리 구조 | **배열(테이블)**만 사용, 재귀 호출 없음    |
| 계산 범위   | 모든 하위 문제를 계산                      |
| 호출 방식   | **반복문 사용** (스택/재귀 없음)           |

### 🧠 Top-down vs Bottom-up 비교 정리

| 비교 항목            | Top-down (Memoization)               | Bottom-up (Tabulation)             |
| -------------------- | ------------------------------------ | ---------------------------------- |
| 핵심 구현 구조       | 재귀 + 캐시                          | 반복문 + DP 테이블                 |
| 계산 흐름            | 큰 문제 → 작은 문제 (필요 시만 계산) | 작은 문제 → 큰 문제 (전체 계산)    |
| 메모리 구조          | 재귀 호출 스택 + 캐시 배열           | 반복문 기반 배열                   |
| 불필요한 계산        | 없음 (요청된 문제만 계산)            | 있음 (모든 문제 계산)              |
| 구현 난이도          | 간단하고 직관적                      | 약간 복잡 (문제 순서 신경 써야 함) |
| 함수 호출 오버헤드   | 존재 (재귀 호출 많으면 비효율)       | 없음                               |
| 스택 오버플로우 위험 | 있음 (재귀 깊이 커지면 스택 초과)    | 없음                               |
| 일반 사용 시기       | 코드 단순화가 중요할 때              | 성능 최적화가 더 중요할 때         |

### 🧩 대표 문제 예시

| 문제                           | Top-down 사용 가능 | Bottom-up 사용 가능 |
| ------------------------------ | ------------------ | ------------------- |
| 피보나치 수열                  | ✅                  | ✅                   |
| 0/1 배낭 문제                  | ✅                  | ✅                   |
| 최소 편집 거리 (Edit Distance) | ✅                  | ✅                   |
| 최장 증가 부분 수열 (LIS)      | ✅                  | ✅                   |
| 행렬 곱 최적화 (Matrix Chain)  | ✅                  | ✅                   |

### ✅ 결론 요약

| 선택 기준                         | 추천 방식 | 이유                                                         |
| --------------------------------- | --------- | ------------------------------------------------------------ |
| 코드가 간단해야 함                | Top-down  | 재귀 기반으로 직관적 구현 가능                               |
| 성능과 시간 최적화가 중요함       | Bottom-up | 반복문 기반으로 함수 호출 오버헤드 없음                      |
| 하위 문제의 순서를 제어할 수 없음 | Top-down  | 필요한 부분만 계산 가능                                      |
| 모든 하위 문제 계산이 필요함      | Bottom-up | 반복문이 빠르고 메모리 절약 가능 (경우에 따라 O(1) 최적화도 가능) |

### 다음 주제

- **Top-down → Bottom-up 변환법**
- **공간 최적화 방법 (O(n) → O(1))**
- **재귀 vs 반복 성능 실험 비교**
   등도 해줄 수 있어.

## 메모이제이션

### 🟦 1. 정의

#### 📌 개념

**메모이제이션**은 **중복되는 계산 결과를 저장(cache)** 해두고,
 **같은 입력이 다시 들어오면 계산하지 않고 저장된 값을 재사용**하는 기법이야.

- “기억해 둔다”는 뜻의 **Memoize**에서 유래
- **Top-down 동적 프로그래밍의 핵심 기법**
- 일반적으로 **재귀 호출**과 함께 사용됨

### 🧠 2. 왜 필요한가?

#### 문제: 피보나치 수열 (재귀식)

```
int fib(int n) {
    if (n <= 1) return n;
    return fib(n - 1) + fib(n - 2);
}
```

- 중복 호출: `fib(5)`는 `fib(3)`을 **2번 이상 호출**
- 이로 인해 시간 복잡도는 **O(2ⁿ)**

#### 해결: **메모이제이션 적용**

```
int dp[100];

int fib(int n) {
    if (n <= 1) return n;
    if (dp[n] != -1) return dp[n];   // 이미 계산한 경우
    return dp[n] = fib(n - 1) + fib(n - 2);
}
```

> ✅ 같은 입력에 대한 **중복 계산을 없앰**
> ✅ 시간 복잡도: **O(n)**

### 🟨 3. 구현 방식

#### 📌 방법

| 항목                   | 설명                                                    |
| ---------------------- | ------------------------------------------------------- |
| 캐시 배열 사용         | 배열/딕셔너리 등을 이용해 계산 결과를 저장              |
| 재귀 호출 내 캐시 확인 | 이미 계산한 값이면 재귀호출하지 않고 결과 반환          |
| 기본값 지정            | 배열을 초기화하고, 계산 안 된 값인지 판단하는 기준 필요 |

#### 📌 언어별 예시

**C 언어 (정수 배열)**

```
int dp[100];

void init() {
    for (int i = 0; i < 100; i++) dp[i] = -1;
}
```

**Python (딕셔너리 기반)**

```
memo = {}

def fib(n):
    if n <= 1:
        return n
    if n in memo:
        return memo[n]
    memo[n] = fib(n-1) + fib(n-2)
    return memo[n]
```

**Python (functools 사용)**

```
from functools import lru_cache

@lru_cache(maxsize=None)
def fib(n):
    if n <= 1: return n
    return fib(n-1) + fib(n-2)
```

### 🧩 4. 메모이제이션이 유리한 조건

| 조건                           | 설명                                             |
| ------------------------------ | ------------------------------------------------ |
| 중복되는 하위 문제 존재        | 같은 입력에 대해 여러 번 계산하게 되는 구조일 때 |
| 입력 범위가 제한됨             | 캐시 메모리(dp 배열)를 만들 수 있을 때           |
| 최적 부분 구조 있음            | 결과가 하위 결과의 결합으로 구성될 수 있을 때    |
| 재귀가 직관적인 문제 구조일 때 | 예: 피보나치, 배낭 문제, 경로 탐색 등            |

### 📈 5. 시간 및 공간 복잡도

| 항목           | 복잡도 설명                                    |
| -------------- | ---------------------------------------------- |
| 시간 복잡도    | **O(n)** 또는 **O(문제 수)**                   |
| 공간 복잡도    | **O(n)** (캐시 배열 + 재귀 스택)               |
| 재귀 깊이 제한 | 언어에 따라 재귀 깊이 제한 있음 (C, Python 등) |

### ✅ 6. 메모이제이션 vs 바텀업 DP

| 항목               | 메모이제이션 (Top-down) | 바텀업 (Bottom-up)       |
| ------------------ | ----------------------- | ------------------------ |
| 구현 방식          | 재귀 + 캐시             | 반복문 + 테이블          |
| 사용한 문제만 계산 | ✅ (필요할 때만 계산)    | ❌ (모든 상태 계산)       |
| 함수 호출 오버헤드 | 있음                    | 없음                     |
| 코드 직관성        | 보통 더 좋음            | 구현 순서를 신경 써야 함 |

### 🎯 대표 적용 문제

| 유형                  | 문제 이름 예시                               |
| --------------------- | -------------------------------------------- |
| 피보나치/계단 오르기  | [Fibonacci, Climbing Stairs]                 |
| 최적 비용 경로        | [Minimum Path Sum, Grid Traveler]            |
| 최장 증가 부분 수열   | [LIS, Longest Path in DAG]                   |
| 편집 거리 계산        | [Edit Distance, String Similarity]           |
| 배낭 문제 / 조합 문제 | [0/1 Knapsack, Coin Change, Target Sum]      |
| 게임 전략             | [Nim, Stone Game, Optimal Strategy for Game] |

### ✅ 결론 요약

| 항목      | 설명                                                 |
| --------- | ---------------------------------------------------- |
| 핵심 개념 | **이전 계산 결과를 저장해 중복 계산을 방지**         |
| 구현 방식 | **재귀 함수 + 캐시 배열** 사용                       |
| 주 사용처 | **Top-down DP** 문제, 하향식 구조에 적합             |
| 효과      | **시간을 줄이고, 공간을 약간 더 사용**하여 효율 향상 |

### 다음 주제

- **Memoization vs Tabulation 실전 성능 비교**
- **메모이제이션 적용 가능한 문제 탐색 기법**
- **트리 구조에서의 메모이제이션 (트리 DP)**
   등도 설명해줄 수 있어.

## 대표 문제: 피보나치, 배낭 문제(Knapsack), LIS, LCS

### ✅ 1. 피보나치 수열 (Fibonacci) — DP 입문 문제

#### 📌 문제 개요

- 피보나치 수: `F(n) = F(n-1) + F(n-2)`
- 기본 재귀는 **지수 시간 O(2ⁿ)** → **메모이제이션** or **DP 테이블**로 최적화

#### 🔤 Bottom-Up 방식 (DP)

```
def fib(n):
    dp = [0] * (n+1)
    dp[0] = 0
    dp[1] = 1
    for i in range(2, n+1):
        dp[i] = dp[i-1] + dp[i-2]
    return dp[n]

print(fib(10))  # 55
```

#### ⏱ 시간복잡도: O(n)

### ✅ 2. 0/1 배낭 문제 (Knapsack Problem)

#### 📌 문제 개요

- 물건 i의 **무게 w[i]**, **가치 v[i]**
- **총 용량 W 이하로** 담되, **가치 합이 최대**가 되도록 선택
- 물건은 **한 번만 선택 가능**

#### 🔤 DP 코드

```
def knapsack(w, v, W):
    n = len(w)
    dp = [[0] * (W+1) for _ in range(n+1)]
    
    for i in range(1, n+1):
        for j in range(W+1):
            if w[i-1] <= j:
                dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i-1]] + v[i-1])
            else:
                dp[i][j] = dp[i-1][j]
    return dp[n][W]

# 예시
weights = [2, 1, 3, 2]
values = [12, 10, 20, 15]
capacity = 5
print(knapsack(weights, values, capacity))  # 출력: 37
```

#### ⏱ 시간복잡도: O(nW)

> `n`: 물건 수, `W`: 최대 용량

### ✅ 3. LIS (Longest Increasing Subsequence)

#### 📌 문제 개요

- 가장 긴 **오름차순 부분 수열**의 길이를 구하라
- 예: `[10, 20, 10, 30, 20, 50]` → `10 → 20 → 30 → 50` → 길이 4

#### 🔤 DP O(n²) 버전

```
def LIS(arr):
    n = len(arr)
    dp = [1] * n
    for i in range(n):
        for j in range(i):
            if arr[j] < arr[i]:
                dp[i] = max(dp[i], dp[j] + 1)
    return max(dp)

print(LIS([10, 20, 10, 30, 20, 50]))  # 4
```

#### 🔥 고급: 이진 탐색으로 O(n log n) 가능 (Patience Sorting)

### ✅ 4. LCS (Longest Common Subsequence)

#### 📌 문제 개요

- 두 문자열 A, B가 있을 때, **공통 부분 수열 중 가장 긴 것**의 길이
- 부분 *수열*: 연속되지 않아도 됨

#### 🔤 DP O(nm)

```
def LCS(a, b):
    n, m = len(a), len(b)
    dp = [[0]*(m+1) for _ in range(n+1)]
    
    for i in range(n):
        for j in range(m):
            if a[i] == b[j]:
                dp[i+1][j+1] = dp[i][j] + 1
            else:
                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])
    return dp[n][m]

print(LCS("ACAYKP", "CAPCAK"))  # 4 (ACAK)
```

#### ⏱ 시간복잡도: O(nm)

### **✅ 각 문제 비교 요약표**

| 문제      | 유형      | 핵심 전략       | 시간복잡도         |
| --------- | --------- | --------------- | ------------------ |
| 피보나치  | 수치 DP   | 점화식 + 테이블 | O(n)               |
| 배낭 문제 | 최적화 DP | 상태(i, w) 정의 | O(nW)              |
| LIS       | 수열 DP   | DP or 이진 탐색 | O(n²) / O(n log n) |
| LCS       | 문자열 DP | 2차원 테이블    | O(nm)              |

# 2.7 백트래킹(Backtracking)

## 상태 공간 트리(State-space tree)

### ✅ 상태 공간 트리란?

**상태 공간 트리(State-space tree)**는 문제 해결 과정을 트리 구조로 표현한 것으로,
 각 노드는 **하나의 상태**를, 가지(branch)는 **결정 또는 선택의 흐름**을 의미해.

📌 즉, "한 문제를 해결하기 위해 가능한 모든 선택의 조합"을 **트리로 시각화한 것**이야.

### ✅ 구조 구성

- **루트 노드 (Root Node)**: 초기 상태
- **자식 노드**: 선택 또는 행동 결과로 도달한 새로운 상태
- **잎 노드 (Leaf Node)**: 종료 상태 (해를 찾았거나, 조건을 만족하지 않음)
- **경로(Path)**: 루트에서 어떤 잎까지의 선택 과정 → 후보 해

### ✅ 예시 1: 순열(Permutation)

배열 `[1, 2, 3]`의 모든 순열을 만드는 문제의 상태 공간 트리:

```
        []
      /  |  \
     1   2   3
    / \  |  / \
   2  3 1 1  2
   |  |     |
   3  2     1
```

각 경로는 하나의 **순열 완성본**을 나타냄.

### ✅ 예시 2: 0/1 Knapsack 상태 공간 트리

무게 제한이 있는 배낭 문제에서 각 물건에 대해 **넣을지 말지 결정**하는 흐름:

```
            [0]
           /    \
     include(1)   exclude(1)
      /     \        /     \
inc(2)   exc(2)  inc(2)  exc(2)
  ...
```

- 깊이가 물건 수 `n`
- 각 노드는 "현재까지 담은 무게와 가치" 상태
- 조건(`w <= capacity`)을 넘는 노드는 더 확장하지 않음 → **가지치기**

### ✅ 예시 3: N-Queen (4x4)

각 행마다 한 칸씩 퀸을 배치하며 충돌 여부 검사:

```
         []
       / | | \
     0  1  2  3     ← 1행 퀸의 위치
    /   ...
충돌 시 확장 안함 (Backtrack)
```

### ✅ 상태 공간 트리 vs 탐색 알고리즘

| 기법        | 상태 공간 트리 사용 여부 | 동작 방식                       |
| ----------- | ------------------------ | ------------------------------- |
| 백트래킹    | ✔️ 사용                   | 조건 불만족 시 트리 가지 중단   |
| 분기 한정법 | ✔️ 사용                   | 가지 확장 전에 상한/하한 평가   |
| DFS / BFS   | ✔️ 사용 가능              | 트리 or 그래프를 순서대로 순회  |
| DP          | ❌ 명시적 트리 X          | 상태 중복 줄이기 (메모이제이션) |

### ✅ 상태 공간 트리 활용 포인트

- **해당 문제의 선택 흐름을 시각화**
- **백트래킹 가지치기 시 어떤 노드를 생략하는지 설명 가능**
- **전체 시간 복잡도 분석** (트리 깊이 × 분기 수)
- **BFS/DFS/우선순위 큐를 적용하는 구조로 변환 가능**

### ✅ 요약

| 용어           | 설명                                            |
| -------------- | ----------------------------------------------- |
| 상태 공간 트리 | 문제 해결을 위한 모든 가능한 상태의 계층적 트리 |
| 노드           | 하나의 상태 (결정의 결과)                       |
| 가지           | 하나의 선택 또는 행동                           |
| 루트           | 초기 상태                                       |
| 잎             | 종료 상태 (해 or 실패)                          |
| 가지치기       | 조건 위배 시 하위 노드 생략                     |

### 📌 실전 팁

- 백트래킹 구현 전에 종이에 상태 공간 트리를 그려보면 **가지치기 포인트**가 명확해져.
- 면접/코딩 테스트에서 “왜 이건 O(2ⁿ)인가요?”라는 질문에 트리 구조로 설명하면 **신뢰도 급상승**.

## 가지치기(Pruning)

### 🌲 가지치기(Pruning)란?

> **탐색 트리에서 불필요한 가지(경우)를 조기에 잘라내는 기법**
>  → **시간과 자원을 절약**하고, **필요한 경우만 탐색**하게 함

#### ❌ "어차피 실패할 길이면, 더 이상 탐색하지 않는다."

### ✅ 가지치기의 목적

| 목적                   | 설명                                                |
| ---------------------- | --------------------------------------------------- |
| **성능 향상**          | 탐색 공간(상태 공간 트리)을 줄여서 실행 시간을 줄임 |
| **불필요한 시도 방지** | 정답이 될 수 없는 경로는 일찍 탈출                  |
| **중복 방지**          | 이미 탐색한 것과 같은 결과를 다시 계산하지 않음     |

### ✅ 예시: 백트래킹 순열 생성에서의 가지치기

예: `[1, 2, 2]`의 모든 순열 생성
 **문제**: `2`가 중복되어 같은 순열이 반복될 수 있음

#### ✂️ 가지치기 조건

- 현재 수 `nums[i]`가 이전 수와 같고, 이전 수가 아직 사용되지 않았다면 **스킵**

#### 🔤 Python 예제 코드

```
def permute_unique(nums):
    result = []
    nums.sort()
    used = [False] * len(nums)
    
    def backtrack(path):
        if len(path) == len(nums):
            result.append(path[:])
            return
        for i in range(len(nums)):
            if used[i]:
                continue
            # 가지치기: 중복 숫자 건너뜀
            if i > 0 and nums[i] == nums[i-1] and not used[i-1]:
                continue
            used[i] = True
            path.append(nums[i])
            backtrack(path)
            path.pop()
            used[i] = False

    backtrack([])
    return result

print(permute_unique([1,2,2]))
```

> 출력: 중복되지 않은 순열만 생성됨

### ✅ 가지치기 사용 사례 요약

| 알고리즘 문제        | 가지치기 예시                             |
| -------------------- | ----------------------------------------- |
| N-Queen              | 퀸이 공격 가능한 위치면 바로 가지 안 뻗음 |
| 배낭 문제 (Knapsack) | 무게 초과하는 경로는 더 안 탐색           |
| 순열/조합            | 이미 사용한 수 or 중복 수는 스킵          |
| 그래프 탐색          | 이미 방문한 노드는 재방문 안 함           |
| 숫자 조합            | 현재 합이 목표보다 크면 더 이상 탐색 X    |

### ✅ Pruning 없이 vs Pruning 사용

| 항목             | 설명                           |
| ---------------- | ------------------------------ |
| **Pruning 없음** | 모든 경우 다 탐색 → 느림       |
| **Pruning 있음** | 불필요한 경우 사전 차단 → 빠름 |

📌 예: N-Queen (8x8)

- Pruning 없이: 약 4천만 개 경로
- Pruning 포함: 약 9만 개 경로
   → 수백 배 차이 발생

### ✅ 실전 구현 팁

- 백트래킹 or DFS에서 조건문(`if`)은 거의 항상 **Pruning 조건**
- `continue`, `return`, `break` 중첩 사용해서 불필요한 탐색 방지
- 탐색 순서 조정(`sort`)을 통해 **가지치기 최적화**

### ✅ 요약

| 항목      | 설명                                       |
| --------- | ------------------------------------------ |
| 정의      | 유망하지 않은 경로를 미리 제거             |
| 효과      | 탐색 트리 크기 축소 → 속도 향상            |
| 사용처    | 백트래킹, DFS, 완전탐색, 최적화 문제 등    |
| 구현 방식 | 조건문 + continue / return / break 등 활용 |
| 핵심 이득 | 시간복잡도 ↓, 메모리 사용 ↓, 불필요 탐색 ↓ |

## 예시: N-Queen, 순열 생성, 조합 탐색

### ✅ 1. N-Queen 문제

#### 📌 문제 정의

`N × N` 체스판 위에 **서로 공격하지 않도록 N개의 퀸(Queen)**을 배치하는 경우의 수를 구하는 문제.

> 조건:
>
> - 같은 **행**, **열**, **대각선**에 두 퀸이 존재하면 안 됨

#### 🔧 DFS + 백트래킹 구현 (C 스타일 의사코드)

```
int N, count = 0;
int col[15]; // col[row] = queen이 놓인 열

int isValid(int row) {
    for (int i = 0; i < row; i++) {
        if (col[i] == col[row] || abs(col[i] - col[row]) == row - i)
            return 0; // 같은 열 또는 대각선
    }
    return 1;
}

void solve(int row) {
    if (row == N) {
        count++;
        return;
    }

    for (int i = 0; i < N; i++) {
        col[row] = i;
        if (isValid(row))
            solve(row + 1);
    }
}
```

#### 🧠 특징

- **Backtracking + DFS**의 전형
- 최적화하려면 비트마스크나 대각선 체크 배열 활용
- 시간 복잡도: **O(N!)**

### ✅ 2. 순열 생성 (Permutation)

#### 📌 문제 정의

서로 다른 N개의 원소로 만들 수 있는 **모든 순열(순서 있는 나열)**을 구하는 문제.

#### 🔧 DFS 방식 구현 (Python 예시)

```
def permute(path, used, nums):
    if len(path) == len(nums):
        print(path)
        return

    for i in range(len(nums)):
        if not used[i]:
            used[i] = True
            permute(path + [nums[i]], used, nums)
            used[i] = False
```

```
nums = [1, 2, 3]
permute([], [False] * len(nums), nums)
```

#### 🧠 특징

- **재귀 호출 → 경로 확장**
- 방문 여부 기록 (`used[]`)
- 전체 경우 수: **N!**

### ✅ 3. 조합 탐색 (Combination)

#### 📌 문제 정의

N개의 원소 중 **R개를 선택하는 모든 조합**을 구하는 문제 (순서 무관)

#### 🔧 DFS 구현 (Python 예시)

```
def combine(start, path, n, r):
    if len(path) == r:
        print(path)
        return

    for i in range(start, n):
        combine(i + 1, path + [i], n, r)
python


코드 복사
combine(0, [], 5, 3)  # 5개 중 3개 선택
```

#### 🧠 특징

- 순서가 **중요하지 않음** → 시작 인덱스를 점점 증가시킴
- 경우의 수: **nCr**
- pruning 적용 가능

### 📊 비교 요약

| 문제 유형   | 핵심 탐색 구조           | 조건/제약                | 경우의 수 | 탐색 방식        |
| ----------- | ------------------------ | ------------------------ | --------- | ---------------- |
| **N-Queen** | 행 기준으로 한 칸씩 놓기 | 같은 열/대각선 배치 불가 | O(N!)     | DFS + 백트래킹   |
| **순열**    | 모든 위치에 원소 배치    | 중복 없이, 순서 중요     | N!        | DFS + used[]     |
| **조합**    | 특정 수의 원소 선택      | 중복 없이, 순서 무관     | nCr       | DFS + 시작인덱스 |

### ✅ 공통 전략 포인트

| 전략           | 설명                                                         |
| -------------- | ------------------------------------------------------------ |
| DFS            | 모든 경우를 탐색하되, 조건 만족 시 다음 단계 진행            |
| 백트래킹       | 유망하지 않은 경로를 조기에 포기 (조건 검사, used 체크 등)   |
| 상태 저장/복원 | 재귀 호출 전후로 상태를 저장하고 되돌리기 (`used`, `board`, `path`) |
| 최적화 기법    | 비트마스크, 가지치기, 메모이제이션 (특정 유형에서) 적용 가능 |

### 다음 주제

- **순열 생성에서 중복 원소 처리 방법**
- **조합의 부분합 문제 확장** (예: 부분 집합의 합, Target Sum)
- **N-Queen 시각화 및 출력**
- **위 문제들을 메모이제이션으로 최적화**
   등으로 확장할 수 있어.

# 2.8 분지한정(Branch and Bound)

### 백트래킹과의 차이점

| 항목                   | 백트래킹 (Backtracking)                                     | 분지한정 (Branch and Bound)                         |
| ---------------------- | ----------------------------------------------------------- | --------------------------------------------------- |
| **주요 목적**          | 해를 **찾는 것** 자체에 초점                                | 해 중에서 **최적해를 찾는 것**에 초점               |
| **탐색 방식**          | DFS (깊이 우선 탐색) 기반                                   | BFS 또는 우선순위 큐 기반 (Best-First)              |
| **가지치기 기준**      | 조건을 만족하지 않으면 되돌아감 (불가능한 경우의 수 제거)   | 현재 노드의 **상한/하한 값을 기준**으로 가지치기    |
| **사용되는 문제 유형** | 모든 해 혹은 하나의 해를 찾는 문제 (예: N-Queen, 순열 생성) | 최적화 문제 (예: 외판원 문제, 배낭 문제의 최적해)   |
| **노드 선택 기준**     | 보통 순서대로 깊게 탐색                                     | 우선순위 큐를 통해 가장 유망한 노드를 먼저 선택     |
| **부분해 평가**        | 조건 만족 여부만 검사 (isPromising)                         | 비용 함수(f(x))를 통해 경계(bound) 계산             |
| **스택/큐 구조**       | 명시적/암시적 스택 사용                                     | 큐 또는 우선순위 큐 사용 (min-heap 등)              |
| **성능 최적화**        | 불필요한 경로 제거를 통해 시간 단축                         | 최적해 탐색을 위해 **비용 평가 + 우선순위 큐** 활용 |

### 🔍 예제 비교

#### ✅ 백트래킹 예제: N-Queen

- 목적: 서로 공격하지 않는 Queen 배치 찾기
- 방식: 조건에 맞는지 검사 후 재귀적으로 시도
- 가지치기: 해당 위치에 두면 충돌하는지 판단해서 return

```
def solve_n_queen(row):
    if row == N:
        print(board)
        return
    for col in range(N):
        if is_promising(row, col):
            board[row] = col
            solve_n_queen(row + 1)
```

#### ✅ 분지한정 예제: 외판원 문제 (TSP)

- 목적: 모든 도시를 한 번씩 방문하고 비용이 **가장 작은 경로** 찾기
- 방식: 부분 경로의 현재 비용 + 예상 비용(하한값) 계산
- 가지치기: 현재 비용이 기존 최적해보다 높으면 버림
- 우선순위 큐 활용 (heap)

```
from queue import PriorityQueue

def tsp_branch_and_bound():
    pq = PriorityQueue()
    pq.put((lower_bound(start), [start]))  # (비용, 경로)
    best_cost = INF
    while not pq.empty():
        cost, path = pq.get()
        if cost >= best_cost:
            continue
        if len(path) == N:
            total_cost = cost + distance[path[-1]][path[0]]
            best_cost = min(best_cost, total_cost)
        else:
            for next in range(N):
                if next not in path:
                    new_path = path + [next]
                    new_cost = cost + distance[path[-1]][next]
                    bound = new_cost + estimate_lower_bound(new_path)
                    if bound < best_cost:
                        pq.put((bound, new_path))
```

### 🎯 핵심 요약

- **백트래킹**은 가능한 모든 경우 중 조건을 만족하는 해를 **찾는 데 중점**.
- **분지한정**은 가능한 해 중에서 **최적의 해를 찾기 위해 경계 조건(bound)을 계산**해서 가지치기를 더 정교하게 함.
- 백트래킹은 "가능한가?"에 중점을 두고, 분지한정은 "더 나은가?"에 집중한다.

### 📘 실전 선택 기준

| 문제 유형                         | 권장 알고리즘 |
| --------------------------------- | ------------- |
| 퍼즐, 조합, 제약 만족             | 백트래킹      |
| 최단 거리, 최소 비용, 최적화 문제 | 분지한정      |

## 우선순위 큐를 활용한 경로 탐색

### 📌 1. 개요

> 우선순위 큐는 각 항목이 **우선순위(priority)**를 가지고 있으며, 가장 높은(또는 낮은) 우선순위를 가진 항목을 먼저 꺼내는 자료구조야.

경로 탐색에서는 보통 **비용(cost)** 또는 **우선 평가값(f(n))**을 기준으로 우선순위를 정해, 가장 유망한 경로를 먼저 탐색하는 전략을 취해.

#### 대표 활용 예시:

- Dijkstra 알고리즘
- A* 탐색
- 분지한정(Branch and Bound)
- 최소 신장 트리 (Prim)
- 외판원 문제(TSP) 최적화

### 🧠 2. 왜 우선순위 큐인가?

경로 탐색에서 가장 먼저 처리해야 할 경로는?

- 지금까지의 **누적 비용이 가장 낮은 경로**
- 또는, **최적 경로에 가까울 것 같은 경로**

이런 판단을 하기 위해 `heapq` (min-heap 기반 우선순위 큐)를 써서 경로를 관리한다.

### 🧮 3. 기본 구조 (Python 예시)

```
import heapq

# (비용, 현재 노드, 경로) 튜플을 넣음
pq = []
heapq.heappush(pq, (0, start_node, [start_node]))

while pq:
    cost, current, path = heapq.heappop(pq)

    if current == goal:
        print("최적 경로:", path)
        break

    for neighbor, edge_cost in graph[current]:
        new_cost = cost + edge_cost
        heapq.heappush(pq, (new_cost, neighbor, path + [neighbor]))
```

### 🔍 4. 적용 예시별 설명

#### ✅ A. Dijkstra 알고리즘 (최단 경로, 음수 가중치 없음)

- **비용 기준 정렬**: 누적 거리 기준으로 우선순위 큐 구성
- **방문한 노드는 다시 방문하지 않음**
- 시간복잡도: `O((V + E) log V)`

```
def dijkstra(start):
    dist = [INF] * N
    dist[start] = 0
    pq = [(0, start)]
    while pq:
        cost, current = heapq.heappop(pq)
        if dist[current] < cost:
            continue
        for neighbor, weight in graph[current]:
            new_cost = cost + weight
            if new_cost < dist[neighbor]:
                dist[neighbor] = new_cost
                heapq.heappush(pq, (new_cost, neighbor))
```

#### ✅ B. A* 알고리즘 (휴리스틱 + 경로 비용)

- `f(n) = g(n) + h(n)`
  - `g(n)`: 시작 → 현재까지 실제 비용
  - `h(n)`: 현재 → 목적지까지의 **추정 비용 (휴리스틱)**
- `h(n)`이 0이면 Dijkstra와 같음

```
def a_star(start, goal, heuristic):
    pq = [(heuristic[start], 0, start, [start])]  # (f, g, node, path)
    while pq:
        f, g, node, path = heapq.heappop(pq)
        if node == goal:
            return path
        for neighbor, cost in graph[node]:
            new_g = g + cost
            new_f = new_g + heuristic[neighbor]
            heapq.heappush(pq, (new_f, new_g, neighbor, path + [neighbor]))
```

#### ✅ C. 분지한정 TSP (최적 경로)

- 현재 경로 비용 + 추정되는 남은 비용 = 우선순위
- 우선순위 큐로 가장 유망한 경로부터 탐색
- 최소 비용 갱신 시 하한보다 큰 경로는 제외

```
heapq.heappush(pq, (lower_bound(path), path))

while pq:
    estimate_cost, path = heapq.heappop(pq)
    if complete(path):
        best = min(best, actual_cost(path))
    else:
        for next_node in candidates(path):
            new_path = path + [next_node]
            est = actual_cost(new_path) + lower_bound(new_path)
            if est < best:
                heapq.heappush(pq, (est, new_path))
```

### 📘 정리: 우선순위 큐 경로 탐색 핵심

| 항목          | 설명                         |
| ------------- | ---------------------------- |
| 구조          | `(비용, 노드, 경로)` 튜플    |
| 정렬 기준     | 비용 or 비용 + 휴리스틱      |
| 탐색 전략     | 가장 유망한 경로 먼저 탐색   |
| 대표 알고리즘 | Dijkstra, A*, Prim, 분지한정 |

### ✅ 시각적 예시

```
Start
  ↓
(비용: 2) → A → (비용: 6) → C → Goal
(비용: 4) → B → (비용: 8) → D → Goal

==> 우선순위 큐는 비용 2 경로 먼저 처리
```

# 2.9 그래프 알고리즘

## 최소 신장 트리 (MST)

### ✅ 1. 최소 신장 트리란?

**정의:**

> 연결 그래프에서 모든 정점을 포함하면서,
>  **간선의 가중치 합이 최소인 트리 형태의 부분 그래프**

#### 🧩 조건

- **모든 정점이 연결되어 있어야 함** (연결 그래프)
- **사이클이 없어야 함** (트리 조건)
- **간선 수는 V - 1**개 (V: 정점 수)

### 🧠 2. 최소 신장 트리의 응용

- **통신망 설계**: 최소 비용으로 네트워크 구축
- **도로망 건설**: 전체 도시 연결, 비용 최소화
- **클러스터링**: MST + 간선 제거 → 군집 나누기
- **이미지 분할/그래픽**: 영역 연결 최소화

### 🔧 3. 주요 알고리즘 비교

| 항목          | Kruskal 알고리즘               | Prim 알고리즘                         |
| ------------- | ------------------------------ | ------------------------------------- |
| 기반 구조     | 간선 중심 (Edge-based)         | 정점 중심 (Vertex-based)              |
| 기본 아이디어 | 가장 가벼운 간선부터 선택      | 연결된 정점에서 가장 가벼운 간선 선택 |
| 구현 구조     | 정렬 + Union-Find              | 우선순위 큐 (Heap)                    |
| 시간복잡도    | O(E log E)                     | O(E log V) (Heap 사용 시)             |
| 적합한 그래프 | **간선 수가 적은 희소 그래프** | **정점 수가 적은 밀집 그래프**        |

### 🪜 4. Kruskal 알고리즘

#### 🧠 핵심 아이디어

1. 모든 간선을 **가중치 기준 정렬**
2. 가장 가벼운 간선부터 하나씩 선택
3. **사이클이 생기지 않으면** 추가
4. `V - 1`개의 간선 선택 시 종료

#### 🔄 알고리즘 흐름

```
간선 정렬 → Union-Find로 사이클 확인 → 트리에 추가
```

#### 📦 코드 (Python)

```
def find(u):
    if parent[u] != u:
        parent[u] = find(parent[u])
    return parent[u]

def union(u, v):
    root_u = find(u)
    root_v = find(v)
    if root_u == root_v:
        return False
    parent[root_v] = root_u
    return True

def kruskal(n, edges):
    parent = list(range(n))
    mst = []
    edges.sort(key=lambda x: x[2])  # (u, v, weight)
    total_cost = 0
    for u, v, w in edges:
        if union(u, v):
            mst.append((u, v, w))
            total_cost += w
            if len(mst) == n - 1:
                break
    return total_cost, mst
```

### 🧮 5. Prim 알고리즘

#### 🧠 핵심 아이디어

1. 아무 정점에서 시작
2. **현재 트리에 연결된 간선 중 최소 가중치** 선택
3. 해당 간선의 정점을 트리에 포함
4. 반복

#### 🔄 알고리즘 흐름

```
시작 정점 선택 → 방문하지 않은 정점으로 가는 최소 간선 선택 → 큐 갱신
```

#### 📦 코드 (Python, heapq 사용)

```
import heapq

def prim(graph, start):
    visited = [False] * len(graph)
    min_heap = [(0, start)]  # (weight, to)
    total_cost = 0
    while min_heap:
        weight, u = heapq.heappop(min_heap)
        if visited[u]:
            continue
        visited[u] = True
        total_cost += weight
        for v, w in graph[u]:
            if not visited[v]:
                heapq.heappush(min_heap, (w, v))
    return total_cost
```

### 📊 6. 시각적 비교

```
[정점 기준 트리 확장]       [간선 기준 트리 확장]
       Prim                    Kruskal
     ┌───┐                    ┌───┐
     │ A │                    │ A │
     └─┬─┘                    └───┘
       ↓                         ↓
   가장 가까운 정점         가장 짧은 간선 (사이클 방지)
```

### 📌 7. MST 관련 고급 주제

- **MST와 이중 연결**: 하나의 간선이 끊겨도 유지되게
- **2차 최소 신장 트리**: MST에서 간선 하나를 바꾸는 방식
- **MST를 기반으로 클러스터링**: 간선 몇 개 제거해서 군집화
- **Borůvka’s Algorithm**: 병렬화에 적합한 MST 알고리즘

### ✅ 예제 문제 유형

| 유형             | 예시 문제                    |
| ---------------- | ---------------------------- |
| 네트워크 구축    | 도시 간 최소 비용 도로 연결  |
| 최소 전력망 구성 | 전봇대 + 전선 최소 설치 비용 |
| 섬 연결 문제     | 섬끼리 다리 연결 최소 비용   |
| 클러스터링       | MST 구성 후 k-1개 간선 제거  |

## 최단 경로 탐색

### ✅ 1. 최단 경로 문제란?

> **그래프 G(V, E)**가 주어졌을 때,
>  두 정점 사이를 **이동할 수 있는 경로 중에서**
>  **총 가중치(비용)의 합이 최소인 경로**를 찾는 문제.

### 🎯 2. 최단 경로 알고리즘 분류표

| 알고리즘                | 음수 가중치 | 음수 사이클   | 방향성    | 시간복잡도        | 특징                   |
| ----------------------- | ----------- | ------------- | --------- | ----------------- | ---------------------- |
| **Dijkstra**            | ❌           | ❌             | 유/무방향 | O(E log V) (heap) | 가장 빠름, 음수 불가   |
| **Bellman-Ford**        | ✅           | ❌             | 유/무방향 | O(VE)             | 음수 허용, 사이클 탐지 |
| **Floyd-Warshall**      | ✅           | ❌             | 유/무방향 | O(V³)             | 모든 쌍 최단 경로      |
| **SPFA (Queue 최적화)** | ✅           | ⚠️ (탐지 가능) | 유방향    | 평균 O(E)         | Bellman-Ford의 개선    |
| **BFS (Unweighted)**    | N/A         | N/A           | 유/무방향 | O(V + E)          | 가중치 없는 그래프     |

### 🧠 3. Dijkstra 알고리즘 (가장 중요)

#### 🔹 조건:

- **음수 가중치가 없을 것**
- **가장 빠르고 효율적인 단일 시작점 최단경로 알고리즘**

#### 🔹 개요:

1. 시작 정점으로부터 거리를 0으로 초기화
2. **가장 짧은 거리**를 가진 정점을 선택 (우선순위 큐 사용)
3. 인접한 노드의 거리 갱신
4. 갱신된 노드는 큐에 다시 삽입

#### 🔹 Python 구현 (heapq 사용)

```
import heapq

def dijkstra(start, graph):
    n = len(graph)
    dist = [float('inf')] * n
    dist[start] = 0
    pq = [(0, start)]  # (거리, 노드)

    while pq:
        cost, u = heapq.heappop(pq)
        if cost > dist[u]:
            continue
        for v, w in graph[u]:
            new_cost = cost + w
            if new_cost < dist[v]:
                dist[v] = new_cost
                heapq.heappush(pq, (new_cost, v))
    return dist
```

### 🧠 4. Bellman-Ford 알고리즘

#### 🔹 특징:

- **음수 간선 허용**
- **음수 사이클 탐지 가능**
- 최단 경로가 **V-1번 Relaxation** 후에도 변하면 **음수 사이클 존재**

#### 🔹 Python 구현

```
def bellman_ford(start, graph, V):
    dist = [float('inf')] * V
    dist[start] = 0
    for _ in range(V - 1):
        for u, v, w in graph:  # edge list
            if dist[u] + w < dist[v]:
                dist[v] = dist[u] + w
    # 음수 사이클 확인
    for u, v, w in graph:
        if dist[u] + w < dist[v]:
            return None  # 음수 사이클 존재
    return dist
```

### 🧠 5. Floyd-Warshall 알고리즘

#### 🔹 특징:

- **모든 정점 간 최단 경로 (All-Pairs)**
- 3중 for문 구조
- **음수 간선 허용** (사이클은 X)

#### 🔹 Python 구현

```
def floyd_warshall(graph):
    n = len(graph)
    dist = [[float('inf')] * n for _ in range(n)]
    for u in range(n):
        dist[u][u] = 0
    for u in range(n):
        for v, w in graph[u]:
            dist[u][v] = w
    for k in range(n):
        for i in range(n):
            for j in range(n):
                dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])
    return dist

```

### 🔍 6. 알고리즘 선택 기준

| 상황                        | 선택 알고리즘    |
| --------------------------- | ---------------- |
| 단일 출발점, 양수 간선      | ✅ Dijkstra       |
| 단일 출발점, 음수 간선 포함 | ✅ Bellman-Ford   |
| 모든 쌍 경로, 정점 수 작음  | ✅ Floyd-Warshall |
| 간선 수 적고 음수 포함      | ✅ SPFA           |
| 가중치 없음                 | ✅ BFS            |

### 🧪 7. 실전 예시 문제 유형

| 문제 유형           | 예시 설명           |
| ------------------- | ------------------- |
| 도시 간 최소 비용   | 그래프 간선 = 비용  |
| 인터넷 지연 최소화  | 지연시간 = 가중치   |
| 운송/물류 최단 거리 | 최적 경로 비용 산출 |
| 음수 간선 존재 시   | Bellman-Ford 사용   |

### 🛠️ 8. 자주 나오는 실수

- Dijkstra에서 **음수 간선 사용 금지**
- Bellman-Ford는 **시간복잡도 크므로** V와 E가 너무 크면 비효율적
- Floyd는 **모든 정점**이 적을 때만 (V ≤ 400 수준)

### ✅ 핵심 요약

| 알고리즘       | 음수 허용   | 목적                     |
| -------------- | ----------- | ------------------------ |
| Dijkstra       | ❌           | 빠른 단일 출발 최단 경로 |
| Bellman-Ford   | ✅           | 음수 간선 + 사이클 감지  |
| Floyd-Warshall | ✅           | 전체 정점 간 최단 경로   |
| BFS            | 가중치 없음 | 정점 간 최단 이동 횟수   |
| SPFA           | ✅           | Bellman의 개선 버전      |

## 위상 정렬 (Topological Sort)

### ✅ 1. 위상 정렬이란?

> **유향 비순환 그래프(DAG, Directed Acyclic Graph)**에서
>  모든 정점을 **선후 관계를 지키면서 나열**하는 정렬 방법.

즉, "선수 과목 → 후수 과목", "작업 A → 작업 B"와 같은 **의존성이 있는 작업들을 정렬**할 때 사용해.

### 📌 2. 전제 조건

- 그래프는 반드시 **DAG**(순환이 없는 방향 그래프)여야 한다.
- 하나 이상의 정점은 진입차수(indegree)가 0이어야 한다.
- **순환(Cycle)**이 있다면 위상 정렬은 **불가능**하다.

### 🧠 3. 위상 정렬의 핵심 아이디어

- **진입 차수(indegree)**: 해당 정점으로 들어오는 간선 수
- 진입 차수가 0인 노드를 큐에 넣고 → 꺼내면서 → 간선 제거
- 제거된 간선 때문에 다른 정점의 진입 차수가 0이 되면 → 다시 큐에 넣음
- 모든 정점이 정렬될 때까지 반복

### 🪜 4. 위상 정렬 알고리즘 (Kahn’s Algorithm: BFS 기반)

#### 💡 시간 복잡도:

`O(V + E)` (정점 + 간선 수만큼)

#### 📦 Python 구현 (heap 없이 기본 큐 사용)

```
from collections import deque, defaultdict

def topological_sort(V, edges):
    graph = defaultdict(list)
    indegree = [0] * V

    for u, v in edges:
        graph[u].append(v)
        indegree[v] += 1

    queue = deque([i for i in range(V) if indegree[i] == 0])
    result = []

    while queue:
        node = queue.popleft()
        result.append(node)
        for neighbor in graph[node]:
            indegree[neighbor] -= 1
            if indegree[neighbor] == 0:
                queue.append(neighbor)

    if len(result) != V:
        raise ValueError("그래프에 사이클이 있습니다.")
    return result
```

### 🌲 5. DFS 기반 위상 정렬 (역후위 순회 + 스택)

```
def topological_sort_dfs(V, edges):
    graph = [[] for _ in range(V)]
    for u, v in edges:
        graph[u].append(v)

    visited = [False] * V
    stack = []

    def dfs(u):
        visited[u] = True
        for v in graph[u]:
            if not visited[v]:
                dfs(v)
        stack.append(u)

    for i in range(V):
        if not visited[i]:
            dfs(i)

    stack.reverse()
    return stack
```

### 🎯 6. 위상 정렬 응용

| 분야          | 활용 예시                      |
| ------------- | ------------------------------ |
| 과목 수강     | 선수 과목 → 후수 과목 순서     |
| 작업 스케줄링 | 의존 관계가 있는 작업 정렬     |
| 컴파일러      | 코드 종속성 해결               |
| 빌드 시스템   | Makefile, 의존성 순서대로 빌드 |
| 패키지 관리자 | 의존성 설치 순서 결정          |

### 🚫 7. 사이클 탐지 (위상 정렬 실패 조건)

- 위상 정렬 결과의 정점 개수 < 전체 정점 수 → **사이클 존재**
- DFS 중 재귀 호출 중 방문된 정점이 다시 등장 → **백엣지(back edge)** → **사이클 존재**

### 🔍 8. 시각적 예시

예시 그래프 (A=0, B=1, C=2, D=3)

```
A → C
B → C
C → D

위상 정렬 결과:
A, B, C, D
또는 B, A, C, D (여러 가지 가능)
```

### ✅ 핵심 요약

| 항목             | 내용                               |
| ---------------- | ---------------------------------- |
| 입력 조건        | DAG (유향 비순환 그래프)           |
| 주요 도구        | indegree 배열, 큐 or 스택          |
| 대표 방식        | Kahn 알고리즘 (BFS), DFS 역후위    |
| 시간 복잡도      | O(V + E)                           |
| 응용 분야        | 스케줄링, 빌드 시스템, 의존성 해결 |
| 사이클 여부 판정 | 결과 정점 수 < V이면 사이클 있음   |

### 다음 주제

- 위상 정렬로 **스케줄링 최적화**
- **진입 차수 배열 시각화**
- **사이클 감지 알고리즘**
- **위상 정렬 + DP (예: 최대 시간 계산)** 등으로 확장할 수 있어.

## 사이클 검출, 유향/무향 그래프에서의 처리 방식

> 그래프 내에 **출발점과 도착점이 같은 경로(순환 구조)**가 존재하는지 판단하는 알고리즘

### ✅ 1. 유향 그래프(Directed Graph)에서의 사이클 검출

#### 🔸 방법 1: DFS + 방문 상태 트래킹 (3색 기법)

| 상태 | 의미                      |
| ---- | ------------------------- |
| 0    | 방문하지 않음             |
| 1    | 방문 중 (재귀 콜 스택 내) |
| 2    | 방문 완료                 |

- **현재 경로 중 다시 방문한 노드가 있다면 → 사이클 존재**

#### 📦 Python 코드

```
def has_cycle_directed(V, graph):
    visited = [0] * V  # 0: 미방문, 1: 방문중, 2: 방문완료

    def dfs(u):
        visited[u] = 1  # 방문 중
        for v in graph[u]:
            if visited[v] == 1:
                return True  # 백엣지 → 사이클
            if visited[v] == 0 and dfs(v):
                return True
        visited[u] = 2  # 방문 완료
        return False

    for i in range(V):
        if visited[i] == 0:
            if dfs(i):
                return True
    return False
```

#### 🔸 방법 2: 위상 정렬 결과 이용 (Kahn’s Algorithm)

- 위상 정렬 결과의 정점 수가 **전체 정점 수보다 작으면 → 사이클 존재**

### ✅ 2. 무향 그래프(Undirected Graph)에서의 사이클 검출

#### 🔸 방법 1: DFS + 부모 노드 확인

- 인접 노드를 탐색할 때, 이미 방문된 노드가 **부모가 아닌 경우** → 사이클 존재

#### 📦 Python 코드

```
def has_cycle_undirected(V, graph):
    visited = [False] * V

    def dfs(u, parent):
        visited[u] = True
        for v in graph[u]:
            if not visited[v]:
                if dfs(v, u):
                    return True
            elif v != parent:
                return True
        return False

    for i in range(V):
        if not visited[i]:
            if dfs(i, -1):
                return True
    return False
```

#### 🔸 방법 2: Union-Find (Disjoint Set)

- 간선을 추가하면서 두 정점이 **이미 같은 집합**이라면 → 사이클 존재

```
def find(u):
    if parent[u] != u:
        parent[u] = find(parent[u])
    return parent[u]

def union(u, v):
    root_u = find(u)
    root_v = find(v)
    if root_u == root_v:
        return False  # 사이클 발생
    parent[root_v] = root_u
    return True

def has_cycle_union_find(V, edges):
    parent = list(range(V))
    for u, v in edges:
        if not union(u, v):
            return True
    return False
```

### 🎯 정리 표: 유향 vs 무향 그래프 사이클 검출 방식

| 항목           | 유향 그래프 (Directed)   | 무향 그래프 (Undirected) |
| -------------- | ------------------------ | ------------------------ |
| 주요 방법      | DFS + 방문 상태 (3색)    | DFS + 부모 확인          |
| 보조 방법      | 위상 정렬 (정점 수 비교) | Union-Find               |
| 대표 특징      | 백엣지 존재 여부         | 부모 노드 외 방문 확인   |
| 사이클 존재 시 | 위상 정렬 실패           | 간선이 루프 구성         |
| 그래프 유형    | DAG 탐지에 필수          | MST 구성 시 검출 필요    |

### 🧪 실전에서의 사용 예

| 문제 유형             | 적용 방식                                 |
| --------------------- | ----------------------------------------- |
| 작업 순서 유효성 검증 | 유향 그래프 + 위상 정렬                   |
| DFS 중 사이클 감지    | 유향 그래프 + 방문 중 중복 방문           |
| 연결 구조 검증        | 무향 그래프 + Union-Find                  |
| 트리 판별             | 무향 그래프 + 간선 수 = N-1 & 사이클 없음 |

### ✅ 핵심 요약

- **유향 그래프**: 백엣지 → 사이클, 위상 정렬 실패
- **무향 그래프**: 방문 중 부모 외 방문 → 사이클, 또는 Union-Find로 루프 판별
- **DFS 기반 방식은 실시간 탐색에 적합**, **Union-Find는 빠르고 간결한 전처리에 적합**

## Union-Find (Disjoint Set Union, DSU)

### ✅ 1. 개념 요약

> **Disjoint Set Union (DSU)** 또는 **Union-Find**는
>  원소들을 **서로소 집합(disjoint sets)**으로 나누고,
>  **집합 합치기(union)** 및 **소속 집합 찾기(find)** 연산을
>  **빠르게 처리**하는 자료구조야.

### 🧠 2. 기본 연산

| 연산          | 설명                                                         |
| ------------- | ------------------------------------------------------------ |
| `find(x)`     | 원소 `x`가 속한 집합의 대표(루트) 원소 반환                  |
| `union(x, y)` | `x`와 `y`가 속한 두 집합을 합침                              |
| `same(x, y)`  | 두 원소가 같은 집합에 속하는지 여부 판단 (`find(x) == find(y)`) |

### 🌲 3. 기본 구조

- 각 원소는 **부모 노드(parent)**를 가짐
- 처음에는 자기 자신이 부모 (`parent[x] = x`)
- **트리 구조**로 집합을 표현

### 📦 4. 기본 구현 (Python)

```
# 초기화
parent = list(range(N))  # 0~N-1

def find(x):
    if parent[x] != x:
        parent[x] = find(parent[x])  # 경로 압축
    return parent[x]

def union(x, y):
    root_x = find(x)
    root_y = find(y)
    if root_x != root_y:
        parent[root_y] = root_x  # 하나를 다른 루트에 붙임
```

### ⚡ 5. 최적화 기법

#### ✅ 1) 경로 압축 (Path Compression)

- `find(x)` 호출 시, **모든 조상 노드를 루트에 직접 연결**
- 깊이를 줄여 다음 `find`가 거의 O(1)에 가까워짐

```
def find(x):
    if parent[x] != x:
        parent[x] = find(parent[x])  # 경로 압축
    return parent[x]
```

#### ✅ 2) Union by Rank / Size

- 항상 **작은 트리를 큰 트리에 붙임**
- 트리의 깊이를 최소화해서 효율 향상

```
rank = [1] * N

def union(x, y):
    root_x = find(x)
    root_y = find(y)
    if root_x == root_y:
        return
    if rank[root_x] < rank[root_y]:
        parent[root_x] = root_y
    else:
        parent[root_y] = root_x
        if rank[root_x] == rank[root_y]:
            rank[root_x] += 1
```

### 🧪 6. 시간 복잡도

> 두 최적화 기법을 함께 사용하면:

- `find`, `union` 모두 **O(α(N))**
  - 여기서 α는 **역 아커만 함수**: 실제로 거의 상수

### 🧩 7. 활용 예시

| 사용처         | 설명                                                    |
| -------------- | ------------------------------------------------------- |
| Kruskal MST    | 간선 연결 시 두 정점이 같은 집합인지 확인 (사이클 방지) |
| 사이클 검출    | 같은 집합이면 이미 연결됨 → 사이클                      |
| 연결 판별      | 같은 네트워크/도시에 속해 있는지 판별                   |
| 네트워크 분리  | 특정 집합으로 분리하거나 병합 가능                      |
| 분리 집합 쿼리 | 오프라인 쿼리 처리 문제 등                              |

### 📘 8. 실전 예시: Kruskal's MST + Union-Find

```
def kruskal(V, edges):
    parent = list(range(V))

    def find(x):
        if parent[x] != x:
            parent[x] = find(parent[x])
        return parent[x]

    def union(x, y):
        root_x = find(x)
        root_y = find(y)
        if root_x == root_y:
            return False  # 사이클 발생
        parent[root_y] = root_x
        return True

    edges.sort(key=lambda x: x[2])  # 가중치 오름차순
    mst_cost = 0
    for u, v, w in edges:
        if union(u, v):
            mst_cost += w
    return mst_cost
```

### 🛠️ 9. 확장 구조 (Advanced Union-Find)

| 기능                | 설명                                     |
| ------------------- | ---------------------------------------- |
| 크기 정보 저장      | 각 집합의 크기를 추적 (e.g., size[root]) |
| 집합 내 데이터 누적 | ex: 각 컴포넌트의 합, 최대값             |
| Undo 가능한 DSU     | 오프라인 쿼리 처리 (Persistent DSU)      |
| 트리 분해           | Offline LCA, Dynamic Connectivity        |

### ✅ 핵심 요약

| 키포인트   | 설명                                |
| ---------- | ----------------------------------- |
| 핵심 연산  | `find`, `union`, `same`             |
| 최적화     | 경로 압축 + rank 기반 union         |
| 시간복잡도 | O(α(N)), 거의 상수                  |
| 대표 활용  | Kruskal, 사이클 검출, 네트워크 판별 |

